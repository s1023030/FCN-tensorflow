{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 15,
>>>>>>> hourglass and regression practice
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import input_data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 16,
>>>>>>> hourglass and regression practice
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height=256\n",
    "img_width=256\n",
    "#Network Parameters\n",
    "n_input=img_height*img_width*3\n",
<<<<<<< HEAD
    "learning_rate=5e-4   #1e-5\n",
    "training_iters=10000\n",
    "batch_size= 4\n",
    "display_step=5\n",
    "dropout=0.9\n",
    "epoch=25\n",
=======
    "learning_rate=2.5e-3  #1e-5\n",
    "training_iters=10000\n",
    "batch_size= 4\n",
    "display_step=10\n",
    "dropout=0.9\n",
    "epoch=45\n",
>>>>>>> hourglass and regression practice
    "\n",
    "\n",
    "#tf graph input\n",
    "x=tf.placeholder(tf.float32,[None,img_height,img_width,3])\n",
    "y=tf.placeholder(tf.float32,[None,64,64,16])\n",
    "keep_prob=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc0=tf.Variable(tf.random_normal([3,3,3,64]),name=\"wc0\")\n",
    "bc0=tf.Variable(tf.random_normal([64]),name=\"bc0\")\n",
    "\n",
    "wc1=tf.Variable(tf.random_normal([3,3,64,128]),name=\"wc1\")\n",
    "bc1=tf.Variable(tf.random_normal([128]),name=\"bc1\")\n",
    "\n",
    "hgw1_1=tf.Variable(tf.random_normal([1,1,128,256]),name=\"hgw1_1\")\n",
    "hgb1_1=tf.Variable(tf.random_normal([256]),name=\"hgb1_1\")\n",
    "hgw1_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"hgw1_2\")\n",
    "hgb1_2=tf.Variable(tf.random_normal([256]),name=\"hgb1_2\")\n",
    "hgw1_3=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw1_3\")\n",
    "hgb1_3=tf.Variable(tf.random_normal([256]),name=\"hgb1_3\")\n",
    "\n",
    "hgw2_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw2_1\")\n",
    "hgb2_1=tf.Variable(tf.random_normal([256]),name=\"hgb2_1\")\n",
    "hgw2_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"hgw2_2\")\n",
    "hgb2_2=tf.Variable(tf.random_normal([256]),name=\"hgb2_2\")\n",
    "hgw2_3=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw2_3\")\n",
    "hgb2_3=tf.Variable(tf.random_normal([256]),name=\"hgb2_3\")\n",
    "\n",
    "hgw3_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw3_1\")\n",
    "hgb3_1=tf.Variable(tf.random_normal([256]),name=\"hgb3_1\")\n",
    "hgw3_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"hgw3_2\")\n",
    "hgb3_2=tf.Variable(tf.random_normal([256]),name=\"hgb3_2\")\n",
    "hgw3_3=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw3_3\")\n",
    "hgb3_3=tf.Variable(tf.random_normal([256]),name=\"hgb3_3\")\n",
    "\n",
    "hgw4_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw4_1\")\n",
    "hgb4_1=tf.Variable(tf.random_normal([256]),name=\"hgb4_1\")\n",
    "hgw4_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"hgw4_2\")\n",
    "hgb4_2=tf.Variable(tf.random_normal([256]),name=\"hgb4_2\")\n",
    "hgw4_3=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw4_3\")\n",
    "hgb4_3=tf.Variable(tf.random_normal([256]),name=\"hgb4_3\")\n",
    "\n",
    "hgw5_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw5_1\")\n",
    "hgb5_1=tf.Variable(tf.random_normal([256]),name=\"hgb5_1\")\n",
    "hgw5_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"hgw5_2\")\n",
    "hgb5_2=tf.Variable(tf.random_normal([256]),name=\"hgb5_2\")\n",
    "hgw5_3=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw5_3\")\n",
    "hgb5_3=tf.Variable(tf.random_normal([256]),name=\"hgb5_3\")\n",
    "\n",
    "hgw7=tf.Variable(tf.random_normal([3,3,256,128]),name=\"hgw7\")\n",
    "hgb7=tf.Variable(tf.random_normal([128]),name=\"hgb7\")\n",
    "\n",
    "brhgw4_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"brhgw4_1\")\n",
    "brhgb4_1=tf.Variable(tf.random_normal([256]),name=\"brhgb4_1\")\n",
    "brhgw4_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"brhgw4_2\")\n",
    "brhgb4_2=tf.Variable(tf.random_normal([256]),name=\"brhgb4_2\")\n",
    "brhgw4_3=tf.Variable(tf.random_normal([1,1,256,128]),name=\"brhgw4_3\")\n",
    "brhgb4_3=tf.Variable(tf.random_normal([128]),name=\"brhgb4_3\")\n",
    "\n",
    "hgw9=tf.Variable(tf.random_normal([3,3,128,64]),name=\"hgw9\")\n",
    "hgb9=tf.Variable(tf.random_normal([64]),name=\"hgb9\")\n",
    "\n",
    "brhgw3_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"brhgw3_1\")\n",
    "brhgb3_1=tf.Variable(tf.random_normal([256]),name=\"brhgb3_1\")\n",
    "brhgw3_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"brhgw3_2\")\n",
    "brhgb3_2=tf.Variable(tf.random_normal([256]),name=\"brhgb3_2\")\n",
    "brhgw3_3=tf.Variable(tf.random_normal([1,1,256,64]),name=\"brhgw3_3\")\n",
    "brhgb3_3=tf.Variable(tf.random_normal([64]),name=\"brhgb3_3\")\n",
    "\n",
    "hgw11=tf.Variable(tf.random_normal([3,3,64,32]),name=\"hgw11\")\n",
    "hgb11=tf.Variable(tf.random_normal([32]),name=\"hgb11\")\n",
    "\n",
    "brhgw2_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"brhgw2_1\")\n",
    "brhgb2_1=tf.Variable(tf.random_normal([256]),name=\"brhgb2_1\")\n",
    "brhgw2_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"brhgw2_2\")\n",
    "brhgb2_2=tf.Variable(tf.random_normal([256]),name=\"brhgb2_2\")\n",
    "brhgw2_3=tf.Variable(tf.random_normal([1,1,256,32]),name=\"brhgw2_3\")\n",
    "brhgb2_3=tf.Variable(tf.random_normal([32]),name=\"brhgb2_3\")\n",
    "\n",
    "hgw13=tf.Variable(tf.random_normal([3,3,32,16]),name=\"hgw13\")\n",
    "hgb13=tf.Variable(tf.random_normal([16]),name=\"hgb13\")\n",
    "\n",
    "brhgw1_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"brhgw1_1\")\n",
    "brhgb1_1=tf.Variable(tf.random_normal([256]),name=\"brhgb1_1\")\n",
    "brhgw1_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"brhgw1_2\")\n",
    "brhgb1_2=tf.Variable(tf.random_normal([256]),name=\"brhgb1_2\")\n",
    "brhgw1_3=tf.Variable(tf.random_normal([1,1,256,16]),name=\"brhgw1_3\")\n",
    "brhgb1_3=tf.Variable(tf.random_normal([16]),name=\"brhgb1_3\")\n",
    "\n",
    "\n",
    "wc2=tf.Variable(tf.random_normal([1,1,16,16]),name=\"wc2\")\n",
    "bc2=tf.Variable(tf.random_normal([16]),name=\"bc2\")\n",
    "\n",
    "wc3=tf.Variable(tf.random_normal([1,1,16,16]),name=\"wc3\")\n",
    "bc3=tf.Variable(tf.random_normal([16]),name=\"bc3\")"
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc0=tf.Variable(tf.random_normal(shape=[3,3,3,64], mean=0, stddev=1),trainable=True,name=\"wc0\")\n",
    "bc0=tf.Variable(tf.random_normal(shape=[64], mean=0, stddev=1),trainable=True,name=\"bc0\")\n",
    "\n",
    "wc1=tf.Variable(tf.random_normal(shape=[3,3,64,128], mean=0, stddev=1),trainable=True,name=\"wc1\")\n",
    "bc1=tf.Variable(tf.random_normal(shape=[128], mean=0, stddev=1),trainable=True,name=\"bc1\")\n",
    "\n",
    "hgw1_1=tf.Variable(tf.random_normal(shape=[1,1,128,256], mean=0, stddev=1),trainable=True,name=\"hgw1_1\")\n",
    "hgb1_1=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb1_1\")\n",
    "hgw1_2=tf.Variable(tf.random_normal(shape=[3,3,256,256], mean=0, stddev=1),trainable=True,name=\"hgw1_2\")\n",
    "hgb1_2=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb1_2\")\n",
    "hgw1_3=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"hgw1_3\")\n",
    "hgb1_3=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb1_3\")\n",
    "\n",
    "hgw2_1=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"hgw2_1\")\n",
    "hgb2_1=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb2_1\")\n",
    "hgw2_2=tf.Variable(tf.random_normal(shape=[3,3,256,256], mean=0, stddev=1),trainable=True,name=\"hgw2_2\")\n",
    "hgb2_2=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb2_2\")\n",
    "hgw2_3=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"hgw2_3\")\n",
    "hgb2_3=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb2_3\")\n",
    "\n",
    "hgw3_1=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"hgw3_1\")\n",
    "hgb3_1=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb3_1\")\n",
    "hgw3_2=tf.Variable(tf.random_normal(shape=[3,3,256,256], mean=0, stddev=1),trainable=True,name=\"hgw3_2\")\n",
    "hgb3_2=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb3_2\")\n",
    "hgw3_3=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"hgw3_3\")\n",
    "hgb3_3=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb3_3\")\n",
    "\n",
    "hgw4_1=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"hgw4_1\")\n",
    "hgb4_1=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb4_1\")\n",
    "hgw4_2=tf.Variable(tf.random_normal(shape=[3,3,256,256], mean=0, stddev=1),trainable=True,name=\"hgw4_2\")\n",
    "hgb4_2=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb4_2\")\n",
    "hgw4_3=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"hgw4_3\")\n",
    "hgb4_3=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb4_3\")\n",
    "\n",
    "hgw5_1=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"hgw5_1\")\n",
    "hgb5_1=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb5_1\")\n",
    "hgw5_2=tf.Variable(tf.random_normal(shape=[3,3,256,256], mean=0, stddev=1),trainable=True,name=\"hgw5_2\")\n",
    "hgb5_2=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb5_2\")\n",
    "hgw5_3=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"hgw5_3\")\n",
    "hgb5_3=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"hgb5_3\")\n",
    "\n",
    "hgw7=tf.Variable(tf.random_normal(shape=[3,3,256,128], mean=0, stddev=1),trainable=True,name=\"hgw7\")\n",
    "hgb7=tf.Variable(tf.random_normal(shape=[128], mean=0, stddev=1),trainable=True,name=\"hgb7\")\n",
    "\n",
    "brhgw4_1=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"brhgw4_1\")\n",
    "brhgb4_1=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"brhgb4_1\")\n",
    "brhgw4_2=tf.Variable(tf.random_normal(shape=[3,3,256,256], mean=0, stddev=1),trainable=True,name=\"brhgw4_2\")\n",
    "brhgb4_2=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"brhgb4_2\")\n",
    "brhgw4_3=tf.Variable(tf.random_normal(shape=[1,1,256,128], mean=0, stddev=1),trainable=True,name=\"brhgw4_3\")\n",
    "brhgb4_3=tf.Variable(tf.random_normal(shape=[128], mean=0, stddev=1),trainable=True,name=\"brhgb4_3\")\n",
    "\n",
    "hgw9=tf.Variable(tf.random_normal(shape=[3,3,128,64], mean=0, stddev=1),trainable=True,name=\"hgw9\")\n",
    "hgb9=tf.Variable(tf.random_normal(shape=[64], mean=0, stddev=1),trainable=True,name=\"hgb9\")\n",
    "\n",
    "brhgw3_1=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"brhgw3_1\")\n",
    "brhgb3_1=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"brhgb3_1\")\n",
    "brhgw3_2=tf.Variable(tf.random_normal(shape=[3,3,256,256], mean=0, stddev=1),trainable=True,name=\"brhgw3_2\")\n",
    "brhgb3_2=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"brhgb3_2\")\n",
    "brhgw3_3=tf.Variable(tf.random_normal(shape=[1,1,256,64], mean=0, stddev=1),trainable=True,name=\"brhgw3_3\")\n",
    "brhgb3_3=tf.Variable(tf.random_normal(shape=[64], mean=0, stddev=1),trainable=True,name=\"brhgb3_3\")\n",
    "\n",
    "hgw11=tf.Variable(tf.random_normal(shape=[3,3,64,32], mean=0, stddev=1),trainable=True,name=\"hgw11\")\n",
    "hgb11=tf.Variable(tf.random_normal(shape=[32], mean=0, stddev=1),trainable=True,name=\"hgb11\")\n",
    "\n",
    "brhgw2_1=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"brhgw2_1\")\n",
    "brhgb2_1=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"brhgb2_1\")\n",
    "brhgw2_2=tf.Variable(tf.random_normal(shape=[3,3,256,256], mean=0, stddev=1),trainable=True,name=\"brhgw2_2\")\n",
    "brhgb2_2=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"brhgb2_2\")\n",
    "brhgw2_3=tf.Variable(tf.random_normal(shape=[1,1,256,32], mean=0, stddev=1),trainable=True,name=\"brhgw2_3\")\n",
    "brhgb2_3=tf.Variable(tf.random_normal(shape=[32], mean=0, stddev=1),trainable=True,name=\"brhgb2_3\")\n",
    "\n",
    "hgw13=tf.Variable(tf.random_normal(shape=[3,3,32,16], mean=0, stddev=1),trainable=True,name=\"hgw13\")\n",
    "hgb13=tf.Variable(tf.random_normal(shape=[16], mean=0, stddev=1),trainable=True,name=\"hgb13\")\n",
    "\n",
    "brhgw1_1=tf.Variable(tf.random_normal(shape=[1,1,256,256], mean=0, stddev=1),trainable=True,name=\"brhgw1_1\")\n",
    "brhgb1_1=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),name=\"brhgb1_1\")\n",
    "brhgw1_2=tf.Variable(tf.random_normal(shape=[3,3,256,256], mean=0, stddev=1),trainable=True,name=\"brhgw1_2\")\n",
    "brhgb1_2=tf.Variable(tf.random_normal(shape=[256], mean=0, stddev=1),trainable=True,name=\"brhgb1_2\")\n",
    "brhgw1_3=tf.Variable(tf.random_normal(shape=[1,1,256,16], mean=0, stddev=1),trainable=True,name=\"brhgw1_3\")\n",
    "brhgb1_3=tf.Variable(tf.random_normal(shape=[16], mean=0, stddev=1),trainable=True,name=\"brhgb1_3\")\n",
    "\n",
    "\n",
    "wc2=tf.Variable(tf.random_normal(shape=[1,1,16,16], mean=0, stddev=1),trainable=True,name=\"wc2\")\n",
    "bc2=tf.Variable(tf.random_normal(shape=[16], mean=0, stddev=1),trainable=True,name=\"bc2\")\n",
    "\n",
    "wc3=tf.Variable(tf.random_normal(shape=[1,1,16,16], mean=0, stddev=1),trainable=True,name=\"wc3\")\n",
    "bc3=tf.Variable(tf.random_normal(shape=[16], mean=0, stddev=1),trainable=True,name=\"bc3\")"
>>>>>>> hourglass and regression practice
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 18,
>>>>>>> hourglass and regression practice
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(img,w,b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img,w,strides=[1,1,1,1],padding='SAME'),b))\n",
    "def max_pool(img,k):\n",
    "    return tf.nn.max_pool(img,ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')\n",
    "def cv_bottleneck(img,w1,b1,w2,b2,w3,b3):\n",
    "    tmp_bn_1=conv2d(img,w1,b1)\n",
    "    tmp_bn_2=conv2d(tmp_bn_1,w2,b2)\n",
    "    tmp_bn_3=conv2d(tmp_bn_2,w3,b3)\n",
    "    return tmp_bn_3"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 19,
>>>>>>> hourglass and regression practice
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct model\n",
    "x=x/255.0\n",
    "conv0=conv2d(x,wc0,bc0)\n",
    "conv0_pooling=max_pool(conv0,k=2)        #128\n",
<<<<<<< HEAD
    "conv0_pooling=tf.nn.dropout(conv0_pooling,keep_prob)\n",
    "\n",
    "conv1=conv2d(conv0_pooling,wc1,bc1)\n",
    "conv1_pooling=max_pool(conv1,k=2)      #64\n",
    "conv1_pooling=tf.nn.dropout(conv1_pooling,keep_prob)\n",
    "\n",
    "hg_cv_1=cv_bottleneck(conv1_pooling,hgw1_1,hgb1_1,hgw1_2,hgb1_2,hgw1_3,hgb1_3)\n",
    "hg_cv_1_pooling=max_pool(hg_cv_1,k=2) #32\n",
    "hg_cv_1_pooling=tf.nn.dropout(hg_cv_1_pooling,keep_prob)\n",
    "    \n",
    "hg_cv_2=cv_bottleneck(hg_cv_1_pooling,hgw2_1,hgb2_1,hgw2_2,hgb2_2,hgw2_3,hgb2_3)\n",
    "hg_cv_2_pooling=max_pool(hg_cv_2,k=2) #16\n",
    "hg_cv_2_pooling=tf.nn.dropout(hg_cv_2_pooling,keep_prob)\n",
    "\n",
    "hg_cv_3=cv_bottleneck(hg_cv_2_pooling,hgw3_1,hgb3_1,hgw3_2,hgb3_2,hgw3_3,hgb3_3)\n",
    "hg_cv_3_pooling=max_pool(hg_cv_3,k=2) #8\n",
    "hg_cv_3_pooling=tf.nn.dropout(hg_cv_3_pooling,keep_prob)\n",
    "\n",
    "hg_cv_4=cv_bottleneck(hg_cv_3_pooling,hgw4_1,hgb4_1,hgw4_2,hgb4_2,hgw4_3,hgb4_3)\n",
    "hg_cv_4_pooling=max_pool(hg_cv_4,k=2) #4\n",
    "hg_cv_4_pooling=tf.nn.dropout(hg_cv_4_pooling,keep_prob)\n",
=======
    "#conv0_pooling=tf.nn.dropout(conv0_pooling,keep_prob)\n",
    "\n",
    "conv1=conv2d(conv0_pooling,wc1,bc1)\n",
    "conv1_pooling=max_pool(conv1,k=2)      #64\n",
    "#conv1_pooling=tf.nn.dropout(conv1_pooling,keep_prob)\n",
    "\n",
    "hg_cv_1=cv_bottleneck(conv1_pooling,hgw1_1,hgb1_1,hgw1_2,hgb1_2,hgw1_3,hgb1_3)\n",
    "hg_cv_1_pooling=max_pool(hg_cv_1,k=2) #32\n",
    "#hg_cv_1_pooling=tf.nn.dropout(hg_cv_1_pooling,keep_prob)\n",
    "    \n",
    "hg_cv_2=cv_bottleneck(hg_cv_1_pooling,hgw2_1,hgb2_1,hgw2_2,hgb2_2,hgw2_3,hgb2_3)\n",
    "hg_cv_2_pooling=max_pool(hg_cv_2,k=2) #16\n",
    "#hg_cv_2_pooling=tf.nn.dropout(hg_cv_2_pooling,keep_prob)\n",
    "\n",
    "hg_cv_3=cv_bottleneck(hg_cv_2_pooling,hgw3_1,hgb3_1,hgw3_2,hgb3_2,hgw3_3,hgb3_3)\n",
    "hg_cv_3_pooling=max_pool(hg_cv_3,k=2) #8\n",
    "#hg_cv_3_pooling=tf.nn.dropout(hg_cv_3_pooling,keep_prob)\n",
    "\n",
    "hg_cv_4=cv_bottleneck(hg_cv_3_pooling,hgw4_1,hgb4_1,hgw4_2,hgb4_2,hgw4_3,hgb4_3)\n",
    "hg_cv_4_pooling=max_pool(hg_cv_4,k=2) #4\n",
    "#hg_cv_4_pooling=tf.nn.dropout(hg_cv_4_pooling,keep_prob)\n",
>>>>>>> hourglass and regression practice
    "#########################################################################################\n",
    "'''\n",
    "hg_cv_5=tf.nn.relu(tf.nn.bias_add(cv_bottleneck(hg_cv_4_pooling,hgw5_1,hgw5_2,hgw5_3),hgb5))\n",
    "hg_cv_5=tf.nn.dropout(hg_cv_5,keep_prob)\n",
    "\n",
    "hg_cv_6=tf.nn.relu(tf.nn.bias_add(cv_bottleneck(hg_cv_5,hgw6_1,hgw6_2,hgw6_3),hgb6))\n",
    "hg_cv_6=tf.nn.dropout(hg_cv_6,keep_prob)\n",
    "\n",
    "hg_cv_7=tf.nn.relu(tf.nn.bias_add(cv_bottleneck(hg_cv_6,hgw6_1,hgw6_2,hgw6_3),hgb6))\n",
    "hg_cv_7=tf.nn.dropout(hg_cv_7,keep_prob)'''\n",
    "\n",
    "hg_cv_5=cv_bottleneck(hg_cv_4_pooling,hgw5_1,hgb5_1,hgw5_2,hgb5_2,hgw5_3,hgb5_3)\n",
<<<<<<< HEAD
    "hg_cv_5=tf.nn.dropout(hg_cv_5,keep_prob)\n",
=======
    "#hg_cv_5=tf.nn.dropout(hg_cv_5,keep_prob)\n",
>>>>>>> hourglass and regression practice
    "\n",
    "#hg_cv_6=conv2d(hg_cv_5,hgw6,hgb6)\n",
    "#hg_cv_6=tf.nn.dropout(hg_cv_6,keep_prob)\n",
    "\n",
    "\n",
    "#########################################################################################  \n",
    "hg_cv_7=conv2d(hg_cv_5,hgw7,hgb7)\n",
<<<<<<< HEAD
    "hg_cv_7=tf.nn.dropout(hg_cv_7,keep_prob)\n",
    "hg_cv_8=tf.image.resize_bilinear(hg_cv_7,hg_cv_4.get_shape().as_list()[1:3])   #8\n",
    "brhg_cv_4=cv_bottleneck(hg_cv_4,brhgw4_1,brhgb4_1,brhgw4_2,brhgb4_2,brhgw4_3,brhgb4_3)\n",
    "hg_add_1=tf.add(hg_cv_8,brhg_cv_4)\n",
    "hg_add_1=tf.nn.dropout(hg_add_1,keep_prob)\n",
    "\n",
    "hg_cv_9=conv2d(hg_add_1,hgw9,hgb9)\n",
    "hg_cv_9=tf.nn.dropout(hg_cv_9,keep_prob)\n",
    "hg_cv_10=tf.image.resize_bilinear(hg_cv_9,hg_cv_3.get_shape().as_list()[1:3])   #16\n",
    "brhg_cv_3=cv_bottleneck(hg_cv_3,brhgw3_1,brhgb3_1,brhgw3_2,brhgb3_2,brhgw3_3,brhgb3_3)\n",
    "hg_add_2=tf.add(hg_cv_10,brhg_cv_3)\n",
    "hg_add_2=tf.nn.dropout(hg_add_2,keep_prob)\n",
    "\n",
    "hg_cv_11=conv2d(hg_add_2,hgw11,hgb11)\n",
    "hg_cv_11=tf.nn.dropout(hg_cv_11,keep_prob)\n",
    "hg_cv_12=tf.image.resize_bilinear(hg_cv_11,hg_cv_2.get_shape().as_list()[1:3])   #32\n",
    "brhg_cv_2=cv_bottleneck(hg_cv_2,brhgw2_1,brhgb2_1,brhgw2_2,brhgb2_2,brhgw2_3,brhgb2_3)\n",
    "hg_add_3=tf.add(hg_cv_12,brhg_cv_2)\n",
    "hg_add_3=tf.nn.dropout(hg_add_3,keep_prob)\n",
    "\n",
    "hg_cv_13=conv2d(hg_add_3,hgw13,hgb13)\n",
    "hg_cv_13=tf.nn.dropout(hg_cv_13,keep_prob)\n",
    "hg_cv_14=tf.image.resize_bilinear(hg_cv_13,hg_cv_1.get_shape().as_list()[1:3])   #32\n",
    "brhg_cv_1=cv_bottleneck(hg_cv_1,brhgw1_1,brhgb1_1,brhgw1_2,brhgb1_2,brhgw1_3,brhgb1_3)\n",
    "hg_add_4=tf.add(hg_cv_14,brhg_cv_1)\n",
    "hg_add_4=tf.nn.dropout(hg_add_4,keep_prob)\n",
=======
    "#hg_cv_7=tf.nn.dropout(hg_cv_7,keep_prob)\n",
    "hg_cv_8=tf.image.resize_bilinear(hg_cv_7,hg_cv_4.get_shape().as_list()[1:3])   #8\n",
    "brhg_cv_4=cv_bottleneck(hg_cv_4,brhgw4_1,brhgb4_1,brhgw4_2,brhgb4_2,brhgw4_3,brhgb4_3)\n",
    "hg_add_1=tf.add(hg_cv_8,brhg_cv_4)\n",
    "#hg_add_1=tf.nn.dropout(hg_add_1,keep_prob)\n",
    "\n",
    "hg_cv_9=conv2d(hg_add_1,hgw9,hgb9)\n",
    "#hg_cv_9=tf.nn.dropout(hg_cv_9,keep_prob)\n",
    "hg_cv_10=tf.image.resize_bilinear(hg_cv_9,hg_cv_3.get_shape().as_list()[1:3])   #16\n",
    "brhg_cv_3=cv_bottleneck(hg_cv_3,brhgw3_1,brhgb3_1,brhgw3_2,brhgb3_2,brhgw3_3,brhgb3_3)\n",
    "hg_add_2=tf.add(hg_cv_10,brhg_cv_3)\n",
    "#hg_add_2=tf.nn.dropout(hg_add_2,keep_prob)\n",
    "\n",
    "hg_cv_11=conv2d(hg_add_2,hgw11,hgb11)\n",
    "#hg_cv_11=tf.nn.dropout(hg_cv_11,keep_prob)\n",
    "hg_cv_12=tf.image.resize_bilinear(hg_cv_11,hg_cv_2.get_shape().as_list()[1:3])   #32\n",
    "brhg_cv_2=cv_bottleneck(hg_cv_2,brhgw2_1,brhgb2_1,brhgw2_2,brhgb2_2,brhgw2_3,brhgb2_3)\n",
    "hg_add_3=tf.add(hg_cv_12,brhg_cv_2)\n",
    "#hg_add_3=tf.nn.dropout(hg_add_3,keep_prob)\n",
    "\n",
    "hg_cv_13=conv2d(hg_add_3,hgw13,hgb13)\n",
    "#hg_cv_13=tf.nn.dropout(hg_cv_13,keep_prob)\n",
    "hg_cv_14=tf.image.resize_bilinear(hg_cv_13,hg_cv_1.get_shape().as_list()[1:3])   #32\n",
    "brhg_cv_1=cv_bottleneck(hg_cv_1,brhgw1_1,brhgb1_1,brhgw1_2,brhgb1_2,brhgw1_3,brhgb1_3)\n",
    "hg_add_4=tf.add(hg_cv_14,brhg_cv_1)\n",
    "#hg_add_4=tf.nn.dropout(hg_add_4,keep_prob)\n",
>>>>>>> hourglass and regression practice
    "\n",
    "\n",
    "conv2=conv2d(hg_add_4,wc2,bc2)\n",
    "\n",
    "conv3=conv2d(conv2,wc3,bc3)\n",
    "\n",
<<<<<<< HEAD
    "#cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=conv2,labels=y))\n",
    "#cost=tf.reduce_mean(tf.abs(tf.subtract(conv2,y)))\n",
    "loss_h=[]\n",
    "\n",
    "for tmp in range(0,16):\n",
    "    b_2=tf.log(tf.clip_by_value(tf.subtract(1.0,conv3[:,:,:,tmp]),1e-10,1.0))\n",
    "    checkb=tf.check_numerics(b_2,\"b_2\")\n",
    "    b_1=tf.subtract(1.0,y[:,:,:,tmp])\n",
    "    b=tf.multiply(b_1,b_2)\n",
    "    \n",
    "    a_2=tf.log(tf.clip_by_value(conv3[:,:,:,tmp],1e-10,1.0))\n",
    "    checka=tf.check_numerics(a_2,\"s_2\")\n",
=======
    "cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=conv2,labels=y))/batch_size\n",
    "#cost=tf.reduce_mean(tf.abs(tf.subtract(conv2,y)))\n",
    "'''loss_h=[]\n",
    "for tmp in range(0,16):\n",
    "    b_2=tf.log(tf.clip_by_value(tf.subtract(1.0,conv3[:,:,:,tmp]),1e-7,1.0))\n",
    "    b_1=tf.subtract(1.0,y[:,:,:,tmp])\n",
    "    b=tf.multiply(b_1,b_2)\n",
    "    \n",
    "    a_2=tf.log(tf.clip_by_value(conv3[:,:,:,tmp],1e-7,1.0))\n",
>>>>>>> hourglass and regression practice
    "    a_1=y[:,:,:,tmp]\n",
    "    a=tf.multiply(a_1,a_2)\n",
    "    c=tf.add(a,b)\n",
    "    c=tf.multiply(c,-1.0)\n",
    "    loss_h.append(tf.reduce_sum(c))\n",
<<<<<<< HEAD
    "    \n",
    "\n",
    "    #tmp_one=tf.reduce_mean(tf.abs(tf.subtract(y[:,:,:,tmp],conv3[:,:,:,tmp])))\n",
    "    #check=tf.check_numerics(tmp_one,\"tmp_one is nan or inf\")\n",
    "    #loss_h.append(tmp_one)\n",
    "cost=tf.reduce_mean(loss_h)/batch_size\n",
    "optimizer =tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=0.0005, momentum=0.9, epsilon=1e-10).minimize(loss=cost,global_step=tf.train.get_global_step())\n",
=======
    "\n",
    "\n",
    "cost=tf.reduce_mean(loss_h)/batch_size'''\n",
    "    \n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=0.0005, momentum=0.9, epsilon=1e-10)\n",
    "\n",
    "\n",
    "\n",
    "###################tf.clip_by_value#####################\n",
    "'''gradients=optimizer.compute_gradients(cost,tf.trainable_variables())\n",
    "def clipIfNotNone(grad):\n",
    "    if grad is None:\n",
    "        return grad\n",
    "    return tf.clip_by_value(grad, -100000, 100000)\n",
    "capped_gvs = [(clipIfNotNone(grad), var) for grad,var in gradients]\n",
    "train_op = optimizer.apply_gradients(capped_gvs)'''\n",
    "\n",
    "\n",
    "###################tf.clip_by_global_norm#####################\n",
    "gradients, variables = zip(*optimizer.compute_gradients(cost, tf.trainable_variables()))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 10000)\n",
    "train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "\n",
    "\n",
    "\n",
>>>>>>> hourglass and regression practice
    "#optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate,epsilon=1e-8).minimize(loss=cost,global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''saver = tf.train.Saver({\"wc0\":wc0,\"bc0\":bc0,\"wc1\":wc1,\"bc1\":bc1,\n",
    "                        \"wh1_1_1\":wh1_1_1,\"wh1_1_2\":wh1_1_2,\"wh1_1_3\":wh1_1_3,\"bh1_1\":bh1_1,\n",
    "                        \"wh1_2_1\":wh1_2_1,\"wh1_2_2\":wh1_2_2,\"wh1_2_3\":wh1_2_3,\"bh1_2\":bh1_2,\n",
    "                        \"wh1_3_1\":wh1_3_1,\"wh1_3_2\":wh1_3_2,\"wh1_3_3\":wh1_3_3,\"bh1_3\":bh1_3,\n",
    "                        \"wh1_4_1\":wh1_4_1,\"wh1_4_2\":wh1_4_2,\"wh1_4_3\":wh1_4_3,\"bh1_4\":bh1_4,\n",
    "                        \"wh1_5_1\":wh1_5_1,\"wh1_5_2\":wh1_5_2,\"wh1_5_3\":wh1_5_3,\"bh1_5\":bh1_5,\n",
    "                        \"wd1\":wd1,\"bd1\":bd1,\"wd2\":wd2,\"bd2\":bd2,\"wc2\":wc2,\"bc2\":bc2,\"wc3\":wc3,\"bc3\":bc3\n",
    "                       },max_to_keep=20)'''\n",
    "saver=tf.train.Saver(max_to_keep=25)\n",
=======
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"wc0\":wc0,\"bc0\":bc0,\"wc1\":wc1,\"bc1\":bc1,\n",
    "                \"hgw1_1\":hgw1_1,\"hgb1_1\":hgb1_1,\"hgw1_2\":hgw1_2,\"hgb1_2\":hgb1_2,\"hgw1_3\":hgw1_3,\"hgb1_3\":hgb1_3,\n",
    "                \"hgw2_1\":hgw2_1,\"hgb2_1\":hgb2_1,\"hgw2_2\":hgw2_2,\"hgb2_2\":hgb2_2,\"hgw2_3\":hgw2_3,\"hgb2_3\":hgb2_3,\n",
    "                \"hgw3_1\":hgw3_1,\"hgb3_1\":hgb3_1,\"hgw3_2\":hgw3_2,\"hgb3_2\":hgb3_2,\"hgw3_3\":hgw3_3,\"hgb3_3\":hgb3_3,\n",
    "                \"hgw4_1\":hgw4_1,\"hgb4_1\":hgb4_1,\"hgw4_2\":hgw4_2,\"hgb4_2\":hgb4_2,\"hgw4_3\":hgw4_3,\"hgb4_3\":hgb4_3,\n",
    "                \"hgw5_1\":hgw5_1,\"hgb5_1\":hgb5_1,\"hgw5_2\":hgw5_2,\"hgb5_2\":hgb5_2,\"hgw5_3\":hgw5_3,\"hgb5_3\":hgb5_3,\n",
    "                \"hgw7\":hgw7,\"hgb7\":hgb7,\n",
    "                \"brhgw4_1\":brhgw4_1,\"brhgb4_1\":brhgb4_1,\"brhgw4_2\":brhgw4_2,\"brhgb4_2\":brhgb4_2,\"brhgw4_3\":brhgw4_3,\"brhgb4_3\":brhgb4_3,\n",
    "                \"hgw9\":hgw9,\"hgb9\":hgb9,\n",
    "                \"brhgw3_1\":brhgw3_1,\"brhgb3_1\":brhgb3_1,\"brhgw3_2\":brhgw3_2,\"brhgb3_2\":brhgb3_2,\"brhgw3_3\":brhgw3_3,\"brhgb3_3\":brhgb3_3,\n",
    "                \"hgw11\":hgw11,\"hgb11\":hgb11,\n",
    "                \"brhgw2_1\":brhgw2_1,\"brhgb2_1\":brhgb2_1,\"brhgw2_2\":brhgw2_2,\"brhgb2_2\":brhgb2_2,\"brhgw2_3\":brhgw2_3,\"brhgb2_3\":brhgb2_3,\n",
    "                \"hgw13\":hgw13,\"hgb13\":hgb13,\n",
    "                \"brhgw1_1\":brhgw1_1,\"brhgb1_1\":brhgb1_1,\"brhgw1_2\":brhgw1_2,\"brhgb1_2\":brhgb1_2,\"brhgw1_3\":brhgw1_3,\"brhgb1_3\":brhgb1_3,\n",
    "                \"wc2\":wc2,\"bc2\":bc2,\n",
    "                \"wc3\":wc3,\"bc3\":bc3\n",
    "                       },max_to_keep=45)\n",
    "#saver=tf.train.Saver(max_to_keep=45)\n",
>>>>>>> hourglass and regression practice
    "init=tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0       !!!!!!!!!!!!\n",
      "Iter 0, Minibatch Loss=1573615588768625575331889152.000000\n",
      "Iter 40, Minibatch Loss=1423435775324120656304406528.000000\n",
      "Iter 80, Minibatch Loss=1629901179730043697281105920.000000\n",
      "Iter 120, Minibatch Loss=1249192848318248280737710080.000000\n",
      "Iter 160, Minibatch Loss=1336606065525454258789416960.000000\n",
      "Iter 200, Minibatch Loss=920470024250336699985428480.000000\n",
      "Iter 240, Minibatch Loss=1152304497911069538917548032.000000\n",
      "Iter 280, Minibatch Loss=1131099153941601166114684928.000000\n",
      "Iter 320, Minibatch Loss=1541178686415462110093901824.000000\n",
      "Iter 360, Minibatch Loss=1326399260668541879365664768.000000\n",
      "Iter 400, Minibatch Loss=1422486874808969036969279488.000000\n",
      "Iter 440, Minibatch Loss=1368371801542239875701407744.000000\n",
      "Iter 480, Minibatch Loss=1233823168729986072007671808.000000\n",
      "Iter 520, Minibatch Loss=1422399806176941127885651968.000000\n",
      "Iter 560, Minibatch Loss=1604010953061663457073430528.000000\n",
      "Iter 600, Minibatch Loss=1386767485028353399278534656.000000\n",
      "Iter 640, Minibatch Loss=1348986487130059982099185664.000000\n",
      "Iter 680, Minibatch Loss=1224158845722793343077842944.000000\n",
      "Iter 720, Minibatch Loss=1497926089077001260552421376.000000\n",
      "Iter 760, Minibatch Loss=1282821410338573383010091008.000000\n",
      "Iter 800, Minibatch Loss=1420677470576267014470369280.000000\n",
      "Iter 840, Minibatch Loss=929472773228069909556101120.000000\n",
      "Iter 880, Minibatch Loss=935879475018821826508750848.000000\n",
      "Iter 920, Minibatch Loss=1439822091871773145843105792.000000\n",
      "Iter 960, Minibatch Loss=1108116724435047909947342848.000000\n",
      "Iter 1000, Minibatch Loss=1153508701364201298447040512.000000\n",
      "Iter 1040, Minibatch Loss=1409368583441414931594870784.000000\n",
      "Iter 1080, Minibatch Loss=1048749272834723280628744192.000000\n",
      "Iter 1120, Minibatch Loss=1346064818016689568476037120.000000\n",
      "Iter 1160, Minibatch Loss=1634027642592356229139398656.000000\n",
      "Iter 1200, Minibatch Loss=1358237603070001617072816128.000000\n",
      "Iter 1240, Minibatch Loss=1298155377030308890048200704.000000\n",
      "Iter 1280, Minibatch Loss=1197144843405442716985720832.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a58b4e0578b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m#gr_print = sess.run(capped_gvs,feed_dict={x:X ,y:Y,keep_prob:dropout})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;31m#print(gr_print)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mdisplay_step\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gatl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gatl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gatl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gatl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gatl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gatl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
>>>>>>> hourglass and regression practice
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess,\"model/model_hg_15.ckpt\")\n",
    "    #saver = tf.train.Saver()\n",
    "    #saver.restore(sess,\"model/model_hg_14_6.ckpt\")\n",
    "    for i in range(0,epoch):\n",
    "        print(\"epoch:\"+str(i)+\"       !!!!!!!!!!!!\")\n",
    "        step=0\n",
    "        now_at=0\n",
    "        while step*batch_size<training_iters:\n",
    "            X=input_data.x_next_batch(\n",
    "                    img_dir_path='mpii_human_pose_v1\\\\output_images256x256\\\\',\n",
<<<<<<< HEAD
    "                    index_path='train_data\\\\new_data_100x200.json',\n",
=======
    "                    index_path='train_data\\\\new_data_64x64.json',\n",
>>>>>>> hourglass and regression practice
    "                    img_height=img_height,img_width=img_width,\n",
    "                    batch_size=batch_size,now_at=now_at)\n",
    "            Y=input_data.y_next_batch(\n",
    "                    img_dir_path='mpii_human_pose_v1\\\\output_images256x256\\\\',\n",
    "                    index_path='train_data\\\\new_data_64x64.json',\n",
    "                    img_height=64,img_width=64,\n",
    "                    batch_size=batch_size,now_at=now_at)\n",
<<<<<<< HEAD
    "            sess.run(optimizer,feed_dict = {x:X,y:Y,keep_prob:dropout})\n",
    "            #msg=sess.run([checka],feed_dict = {x:X,y:Y,keep_prob:dropout})\n",
    "            #print(msg[0])\n",
    "            #print(msg[1])\n",
=======
    "            #gr_print = sess.run(capped_gvs,feed_dict={x:X ,y:Y,keep_prob:dropout})\n",
    "            #print(gr_print)\n",
    "            sess.run(train_op,feed_dict = {x:X,y:Y,keep_prob:dropout})\n",
>>>>>>> hourglass and regression practice
    "            loss=sess.run(cost,feed_dict = {x:X,y:Y,keep_prob:1.})\n",
    "            if step%display_step==0:\n",
    "                print(\"Iter \"+str(step*batch_size)+\", Minibatch Loss=\"+\"{:.6f}\".format(loss))\n",
    "            step+=1\n",
    "            now_at+=batch_size\n",
    "        save_path = saver.save(sess, \"model/model_hg_\"+str(i)+\".ckpt\")\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
