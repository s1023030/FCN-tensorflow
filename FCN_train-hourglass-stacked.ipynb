{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height=256\n",
    "img_width=256\n",
    "#Network Parameters\n",
    "n_input=img_height*img_width*3\n",
    "learning_rate=5e-4   #1e-5\n",
    "training_iters=10000\n",
    "batch_size= 4\n",
    "display_step=5\n",
    "dropout=0.9\n",
    "epoch=25\n",
    "\n",
    "\n",
    "#tf graph input\n",
    "x=tf.placeholder(tf.float32,[None,img_height,img_width,3])\n",
    "y=tf.placeholder(tf.float32,[None,64,64,16])\n",
    "keep_prob=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc0=tf.Variable(tf.random_normal([3,3,3,64]),name=\"wc0\")\n",
    "bc0=tf.Variable(tf.random_normal([64]),name=\"bc0\")\n",
    "\n",
    "wc1=tf.Variable(tf.random_normal([3,3,64,128]),name=\"wc1\")\n",
    "bc1=tf.Variable(tf.random_normal([128]),name=\"bc1\")\n",
    "\n",
    "hgw1_1=tf.Variable(tf.random_normal([1,1,128,256]),name=\"hgw1_1\")\n",
    "hgb1_1=tf.Variable(tf.random_normal([256]),name=\"hgb1_1\")\n",
    "hgw1_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"hgw1_2\")\n",
    "hgb1_2=tf.Variable(tf.random_normal([256]),name=\"hgb1_2\")\n",
    "hgw1_3=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw1_3\")\n",
    "hgb1_3=tf.Variable(tf.random_normal([256]),name=\"hgb1_3\")\n",
    "\n",
    "hgw2_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw2_1\")\n",
    "hgb2_1=tf.Variable(tf.random_normal([256]),name=\"hgb2_1\")\n",
    "hgw2_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"hgw2_2\")\n",
    "hgb2_2=tf.Variable(tf.random_normal([256]),name=\"hgb2_2\")\n",
    "hgw2_3=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw2_3\")\n",
    "hgb2_3=tf.Variable(tf.random_normal([256]),name=\"hgb2_3\")\n",
    "\n",
    "hgw3_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw3_1\")\n",
    "hgb3_1=tf.Variable(tf.random_normal([256]),name=\"hgb3_1\")\n",
    "hgw3_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"hgw3_2\")\n",
    "hgb3_2=tf.Variable(tf.random_normal([256]),name=\"hgb3_2\")\n",
    "hgw3_3=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw3_3\")\n",
    "hgb3_3=tf.Variable(tf.random_normal([256]),name=\"hgb3_3\")\n",
    "\n",
    "hgw4_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw4_1\")\n",
    "hgb4_1=tf.Variable(tf.random_normal([256]),name=\"hgb4_1\")\n",
    "hgw4_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"hgw4_2\")\n",
    "hgb4_2=tf.Variable(tf.random_normal([256]),name=\"hgb4_2\")\n",
    "hgw4_3=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw4_3\")\n",
    "hgb4_3=tf.Variable(tf.random_normal([256]),name=\"hgb4_3\")\n",
    "\n",
    "hgw5_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw5_1\")\n",
    "hgb5_1=tf.Variable(tf.random_normal([256]),name=\"hgb5_1\")\n",
    "hgw5_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"hgw5_2\")\n",
    "hgb5_2=tf.Variable(tf.random_normal([256]),name=\"hgb5_2\")\n",
    "hgw5_3=tf.Variable(tf.random_normal([1,1,256,256]),name=\"hgw5_3\")\n",
    "hgb5_3=tf.Variable(tf.random_normal([256]),name=\"hgb5_3\")\n",
    "\n",
    "hgw7=tf.Variable(tf.random_normal([3,3,256,128]),name=\"hgw7\")\n",
    "hgb7=tf.Variable(tf.random_normal([128]),name=\"hgb7\")\n",
    "\n",
    "brhgw4_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"brhgw4_1\")\n",
    "brhgb4_1=tf.Variable(tf.random_normal([256]),name=\"brhgb4_1\")\n",
    "brhgw4_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"brhgw4_2\")\n",
    "brhgb4_2=tf.Variable(tf.random_normal([256]),name=\"brhgb4_2\")\n",
    "brhgw4_3=tf.Variable(tf.random_normal([1,1,256,128]),name=\"brhgw4_3\")\n",
    "brhgb4_3=tf.Variable(tf.random_normal([128]),name=\"brhgb4_3\")\n",
    "\n",
    "hgw9=tf.Variable(tf.random_normal([3,3,128,64]),name=\"hgw9\")\n",
    "hgb9=tf.Variable(tf.random_normal([64]),name=\"hgb9\")\n",
    "\n",
    "brhgw3_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"brhgw3_1\")\n",
    "brhgb3_1=tf.Variable(tf.random_normal([256]),name=\"brhgb3_1\")\n",
    "brhgw3_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"brhgw3_2\")\n",
    "brhgb3_2=tf.Variable(tf.random_normal([256]),name=\"brhgb3_2\")\n",
    "brhgw3_3=tf.Variable(tf.random_normal([1,1,256,64]),name=\"brhgw3_3\")\n",
    "brhgb3_3=tf.Variable(tf.random_normal([64]),name=\"brhgb3_3\")\n",
    "\n",
    "hgw11=tf.Variable(tf.random_normal([3,3,64,32]),name=\"hgw11\")\n",
    "hgb11=tf.Variable(tf.random_normal([32]),name=\"hgb11\")\n",
    "\n",
    "brhgw2_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"brhgw2_1\")\n",
    "brhgb2_1=tf.Variable(tf.random_normal([256]),name=\"brhgb2_1\")\n",
    "brhgw2_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"brhgw2_2\")\n",
    "brhgb2_2=tf.Variable(tf.random_normal([256]),name=\"brhgb2_2\")\n",
    "brhgw2_3=tf.Variable(tf.random_normal([1,1,256,32]),name=\"brhgw2_3\")\n",
    "brhgb2_3=tf.Variable(tf.random_normal([32]),name=\"brhgb2_3\")\n",
    "\n",
    "hgw13=tf.Variable(tf.random_normal([3,3,32,16]),name=\"hgw13\")\n",
    "hgb13=tf.Variable(tf.random_normal([16]),name=\"hgb13\")\n",
    "\n",
    "brhgw1_1=tf.Variable(tf.random_normal([1,1,256,256]),name=\"brhgw1_1\")\n",
    "brhgb1_1=tf.Variable(tf.random_normal([256]),name=\"brhgb1_1\")\n",
    "brhgw1_2=tf.Variable(tf.random_normal([3,3,256,256]),name=\"brhgw1_2\")\n",
    "brhgb1_2=tf.Variable(tf.random_normal([256]),name=\"brhgb1_2\")\n",
    "brhgw1_3=tf.Variable(tf.random_normal([1,1,256,16]),name=\"brhgw1_3\")\n",
    "brhgb1_3=tf.Variable(tf.random_normal([16]),name=\"brhgb1_3\")\n",
    "\n",
    "\n",
    "wc2=tf.Variable(tf.random_normal([1,1,16,16]),name=\"wc2\")\n",
    "bc2=tf.Variable(tf.random_normal([16]),name=\"bc2\")\n",
    "\n",
    "wc3=tf.Variable(tf.random_normal([1,1,16,16]),name=\"wc3\")\n",
    "bc3=tf.Variable(tf.random_normal([16]),name=\"bc3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(img,w,b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img,w,strides=[1,1,1,1],padding='SAME'),b))\n",
    "def max_pool(img,k):\n",
    "    return tf.nn.max_pool(img,ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')\n",
    "def cv_bottleneck(img,w1,b1,w2,b2,w3,b3):\n",
    "    tmp_bn_1=conv2d(img,w1,b1)\n",
    "    tmp_bn_2=conv2d(tmp_bn_1,w2,b2)\n",
    "    tmp_bn_3=conv2d(tmp_bn_2,w3,b3)\n",
    "    return tmp_bn_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct model\n",
    "x=x/255.0\n",
    "conv0=conv2d(x,wc0,bc0)\n",
    "conv0_pooling=max_pool(conv0,k=2)        #128\n",
    "conv0_pooling=tf.nn.dropout(conv0_pooling,keep_prob)\n",
    "\n",
    "conv1=conv2d(conv0_pooling,wc1,bc1)\n",
    "conv1_pooling=max_pool(conv1,k=2)      #64\n",
    "conv1_pooling=tf.nn.dropout(conv1_pooling,keep_prob)\n",
    "\n",
    "hg_cv_1=cv_bottleneck(conv1_pooling,hgw1_1,hgb1_1,hgw1_2,hgb1_2,hgw1_3,hgb1_3)\n",
    "hg_cv_1_pooling=max_pool(hg_cv_1,k=2) #32\n",
    "hg_cv_1_pooling=tf.nn.dropout(hg_cv_1_pooling,keep_prob)\n",
    "    \n",
    "hg_cv_2=cv_bottleneck(hg_cv_1_pooling,hgw2_1,hgb2_1,hgw2_2,hgb2_2,hgw2_3,hgb2_3)\n",
    "hg_cv_2_pooling=max_pool(hg_cv_2,k=2) #16\n",
    "hg_cv_2_pooling=tf.nn.dropout(hg_cv_2_pooling,keep_prob)\n",
    "\n",
    "hg_cv_3=cv_bottleneck(hg_cv_2_pooling,hgw3_1,hgb3_1,hgw3_2,hgb3_2,hgw3_3,hgb3_3)\n",
    "hg_cv_3_pooling=max_pool(hg_cv_3,k=2) #8\n",
    "hg_cv_3_pooling=tf.nn.dropout(hg_cv_3_pooling,keep_prob)\n",
    "\n",
    "hg_cv_4=cv_bottleneck(hg_cv_3_pooling,hgw4_1,hgb4_1,hgw4_2,hgb4_2,hgw4_3,hgb4_3)\n",
    "hg_cv_4_pooling=max_pool(hg_cv_4,k=2) #4\n",
    "hg_cv_4_pooling=tf.nn.dropout(hg_cv_4_pooling,keep_prob)\n",
    "#########################################################################################\n",
    "'''\n",
    "hg_cv_5=tf.nn.relu(tf.nn.bias_add(cv_bottleneck(hg_cv_4_pooling,hgw5_1,hgw5_2,hgw5_3),hgb5))\n",
    "hg_cv_5=tf.nn.dropout(hg_cv_5,keep_prob)\n",
    "\n",
    "hg_cv_6=tf.nn.relu(tf.nn.bias_add(cv_bottleneck(hg_cv_5,hgw6_1,hgw6_2,hgw6_3),hgb6))\n",
    "hg_cv_6=tf.nn.dropout(hg_cv_6,keep_prob)\n",
    "\n",
    "hg_cv_7=tf.nn.relu(tf.nn.bias_add(cv_bottleneck(hg_cv_6,hgw6_1,hgw6_2,hgw6_3),hgb6))\n",
    "hg_cv_7=tf.nn.dropout(hg_cv_7,keep_prob)'''\n",
    "\n",
    "hg_cv_5=cv_bottleneck(hg_cv_4_pooling,hgw5_1,hgb5_1,hgw5_2,hgb5_2,hgw5_3,hgb5_3)\n",
    "hg_cv_5=tf.nn.dropout(hg_cv_5,keep_prob)\n",
    "\n",
    "#hg_cv_6=conv2d(hg_cv_5,hgw6,hgb6)\n",
    "#hg_cv_6=tf.nn.dropout(hg_cv_6,keep_prob)\n",
    "\n",
    "\n",
    "#########################################################################################  \n",
    "hg_cv_7=conv2d(hg_cv_5,hgw7,hgb7)\n",
    "hg_cv_7=tf.nn.dropout(hg_cv_7,keep_prob)\n",
    "hg_cv_8=tf.image.resize_bilinear(hg_cv_7,hg_cv_4.get_shape().as_list()[1:3])   #8\n",
    "brhg_cv_4=cv_bottleneck(hg_cv_4,brhgw4_1,brhgb4_1,brhgw4_2,brhgb4_2,brhgw4_3,brhgb4_3)\n",
    "hg_add_1=tf.add(hg_cv_8,brhg_cv_4)\n",
    "hg_add_1=tf.nn.dropout(hg_add_1,keep_prob)\n",
    "\n",
    "hg_cv_9=conv2d(hg_add_1,hgw9,hgb9)\n",
    "hg_cv_9=tf.nn.dropout(hg_cv_9,keep_prob)\n",
    "hg_cv_10=tf.image.resize_bilinear(hg_cv_9,hg_cv_3.get_shape().as_list()[1:3])   #16\n",
    "brhg_cv_3=cv_bottleneck(hg_cv_3,brhgw3_1,brhgb3_1,brhgw3_2,brhgb3_2,brhgw3_3,brhgb3_3)\n",
    "hg_add_2=tf.add(hg_cv_10,brhg_cv_3)\n",
    "hg_add_2=tf.nn.dropout(hg_add_2,keep_prob)\n",
    "\n",
    "hg_cv_11=conv2d(hg_add_2,hgw11,hgb11)\n",
    "hg_cv_11=tf.nn.dropout(hg_cv_11,keep_prob)\n",
    "hg_cv_12=tf.image.resize_bilinear(hg_cv_11,hg_cv_2.get_shape().as_list()[1:3])   #32\n",
    "brhg_cv_2=cv_bottleneck(hg_cv_2,brhgw2_1,brhgb2_1,brhgw2_2,brhgb2_2,brhgw2_3,brhgb2_3)\n",
    "hg_add_3=tf.add(hg_cv_12,brhg_cv_2)\n",
    "hg_add_3=tf.nn.dropout(hg_add_3,keep_prob)\n",
    "\n",
    "hg_cv_13=conv2d(hg_add_3,hgw13,hgb13)\n",
    "hg_cv_13=tf.nn.dropout(hg_cv_13,keep_prob)\n",
    "hg_cv_14=tf.image.resize_bilinear(hg_cv_13,hg_cv_1.get_shape().as_list()[1:3])   #32\n",
    "brhg_cv_1=cv_bottleneck(hg_cv_1,brhgw1_1,brhgb1_1,brhgw1_2,brhgb1_2,brhgw1_3,brhgb1_3)\n",
    "hg_add_4=tf.add(hg_cv_14,brhg_cv_1)\n",
    "hg_add_4=tf.nn.dropout(hg_add_4,keep_prob)\n",
    "\n",
    "\n",
    "conv2=conv2d(hg_add_4,wc2,bc2)\n",
    "\n",
    "conv3=conv2d(conv2,wc3,bc3)\n",
    "\n",
    "#cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=conv2,labels=y))\n",
    "#cost=tf.reduce_mean(tf.abs(tf.subtract(conv2,y)))\n",
    "loss_h=[]\n",
    "\n",
    "for tmp in range(0,16):\n",
    "    b_2=tf.log(tf.clip_by_value(tf.subtract(1.0,conv3[:,:,:,tmp]),1e-10,1.0))\n",
    "    checkb=tf.check_numerics(b_2,\"b_2\")\n",
    "    b_1=tf.subtract(1.0,y[:,:,:,tmp])\n",
    "    b=tf.multiply(b_1,b_2)\n",
    "    \n",
    "    a_2=tf.log(tf.clip_by_value(conv3[:,:,:,tmp],1e-10,1.0))\n",
    "    checka=tf.check_numerics(a_2,\"s_2\")\n",
    "    a_1=y[:,:,:,tmp]\n",
    "    a=tf.multiply(a_1,a_2)\n",
    "    c=tf.add(a,b)\n",
    "    c=tf.multiply(c,-1.0)\n",
    "    loss_h.append(tf.reduce_sum(c))\n",
    "    \n",
    "\n",
    "    #tmp_one=tf.reduce_mean(tf.abs(tf.subtract(y[:,:,:,tmp],conv3[:,:,:,tmp])))\n",
    "    #check=tf.check_numerics(tmp_one,\"tmp_one is nan or inf\")\n",
    "    #loss_h.append(tmp_one)\n",
    "cost=tf.reduce_mean(loss_h)/batch_size\n",
    "optimizer =tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=0.0005, momentum=0.9, epsilon=1e-10).minimize(loss=cost,global_step=tf.train.get_global_step())\n",
    "#optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate,epsilon=1e-8).minimize(loss=cost,global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''saver = tf.train.Saver({\"wc0\":wc0,\"bc0\":bc0,\"wc1\":wc1,\"bc1\":bc1,\n",
    "                        \"wh1_1_1\":wh1_1_1,\"wh1_1_2\":wh1_1_2,\"wh1_1_3\":wh1_1_3,\"bh1_1\":bh1_1,\n",
    "                        \"wh1_2_1\":wh1_2_1,\"wh1_2_2\":wh1_2_2,\"wh1_2_3\":wh1_2_3,\"bh1_2\":bh1_2,\n",
    "                        \"wh1_3_1\":wh1_3_1,\"wh1_3_2\":wh1_3_2,\"wh1_3_3\":wh1_3_3,\"bh1_3\":bh1_3,\n",
    "                        \"wh1_4_1\":wh1_4_1,\"wh1_4_2\":wh1_4_2,\"wh1_4_3\":wh1_4_3,\"bh1_4\":bh1_4,\n",
    "                        \"wh1_5_1\":wh1_5_1,\"wh1_5_2\":wh1_5_2,\"wh1_5_3\":wh1_5_3,\"bh1_5\":bh1_5,\n",
    "                        \"wd1\":wd1,\"bd1\":bd1,\"wd2\":wd2,\"bd2\":bd2,\"wc2\":wc2,\"bc2\":bc2,\"wc3\":wc3,\"bc3\":bc3\n",
    "                       },max_to_keep=20)'''\n",
    "saver=tf.train.Saver(max_to_keep=25)\n",
    "init=tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess,\"model/model_hg_15.ckpt\")\n",
    "    #saver = tf.train.Saver()\n",
    "    #saver.restore(sess,\"model/model_hg_14_6.ckpt\")\n",
    "    for i in range(0,epoch):\n",
    "        print(\"epoch:\"+str(i)+\"       !!!!!!!!!!!!\")\n",
    "        step=0\n",
    "        now_at=0\n",
    "        while step*batch_size<training_iters:\n",
    "            X=input_data.x_next_batch(\n",
    "                    img_dir_path='mpii_human_pose_v1\\\\output_images256x256\\\\',\n",
    "                    index_path='train_data\\\\new_data_100x200.json',\n",
    "                    img_height=img_height,img_width=img_width,\n",
    "                    batch_size=batch_size,now_at=now_at)\n",
    "            Y=input_data.y_next_batch(\n",
    "                    img_dir_path='mpii_human_pose_v1\\\\output_images256x256\\\\',\n",
    "                    index_path='train_data\\\\new_data_64x64.json',\n",
    "                    img_height=64,img_width=64,\n",
    "                    batch_size=batch_size,now_at=now_at)\n",
    "            sess.run(optimizer,feed_dict = {x:X,y:Y,keep_prob:dropout})\n",
    "            #msg=sess.run([checka],feed_dict = {x:X,y:Y,keep_prob:dropout})\n",
    "            #print(msg[0])\n",
    "            #print(msg[1])\n",
    "            loss=sess.run(cost,feed_dict = {x:X,y:Y,keep_prob:1.})\n",
    "            if step%display_step==0:\n",
    "                print(\"Iter \"+str(step*batch_size)+\", Minibatch Loss=\"+\"{:.6f}\".format(loss))\n",
    "            step+=1\n",
    "            now_at+=batch_size\n",
    "        save_path = saver.save(sess, \"model/model_hg_\"+str(i)+\".ckpt\")\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
