{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height=480\n",
    "img_width=360\n",
    "#Network Parameters\n",
    "n_input=img_height*img_width*3\n",
    "learning_rate=1e-8\n",
    "training_iters=500\n",
    "batch_size=2\n",
    "display_step=1\n",
    "dropout=0.8\n",
    "epoch=2\n",
    "\n",
    "#tf graph input\n",
    "x=tf.placeholder(tf.float32,[None,img_height,img_width,3])\n",
    "y=tf.placeholder(tf.float32,[None,img_height,img_width,16])\n",
    "keep_prob=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(img,w,b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img,w,strides=[1,1,1,1],padding='SAME'),b))\n",
    "def max_pool(img,k):\n",
    "    return tf.nn.max_pool(img,ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')\n",
    "def conv2d_transpose(img,w,b,output_shape):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d_transpose(img,w,output_shape=output_shape,strides=[1,1,1,1],padding='SAME'),b))\n",
    "def cv_bottleneck(img,w1,w2):\n",
    "    tmp_bn_1=tf.nn.relu(tf.nn.conv2d(img,w1,strides=[1,1,1,1],padding='SAME'))\n",
    "    tmp_bn_2=tf.nn.conv2d(tmp_bn_1,w2,strides=[1,1,1,1],padding='SAME')\n",
    "    return (img+tmp_bn_2)\n",
    "def dc_bottleneck(img,w1,w2,output_shape):\n",
    "    output_shape[3]=w1.get_shape().as_list()[2]\n",
    "    tmp_bn_1=tf.nn.relu(tf.nn.conv2d_transpose(img,w1,output_shape=output_shape,strides=[1,1,1,1],padding='SAME'))\n",
    "    output_shape[3]=w2.get_shape().as_list()[2]\n",
    "    tmp_bn_2=tf.nn.conv2d_transpose(tmp_bn_1,w2,output_shape=output_shape,strides=[1,1,1,1],padding='SAME')\n",
    "    return (img+tmp_bn_2)\n",
    "def hourglass(img,wc1_1,wc1_2,bc1,wc2_1,wc2_2,bc2,wc3_1,wc3_2,bc3,wc4_1,wc4_2,bc4,wc5_1,wc5_2,bc5):\n",
    "    tmp_cv_1=tf.nn.relu(tf.nn.bias_add(cv_bottleneck(img,wc1_1,wc1_2),bc1))\n",
    "    tmp_cv_1_pooling=max_pool(tmp_cv_1,k=2)\n",
    "    tmp_cv_1_pooling=tf.nn.dropout(tmp_cv_1_pooling,keep_prob)\n",
    "    \n",
    "    tmp_cv_2=tf.nn.relu(tf.nn.bias_add(cv_bottleneck(tmp_cv_1_pooling,wc2_1,wc2_2),bc2))\n",
    "    tmp_cv_2_pooling=max_pool(tmp_cv_2,k=2)\n",
    "    tmp_cv_2_pooling=tf.nn.dropout(tmp_cv_2_pooling,keep_prob)\n",
    "    \n",
    "    tmp_cv_3=tf.nn.relu(tf.nn.bias_add(cv_bottleneck(tmp_cv_2_pooling,wc3_1,wc3_2),bc3))\n",
    "    tmp_cv_3=tf.nn.dropout(tmp_cv_3,keep_prob)\n",
    "    \n",
    "    tmp_rs_1=tf.image.resize_bilinear(tmp_cv_3,tmp_cv_2.get_shape().as_list()[1:3])\n",
    "    tmpShape=tmp_cv_1_pooling.get_shape().as_list()\n",
    "    tmpShape[0]=batch_size\n",
    "    tmp_dc_1=tf.nn.relu(tf.nn.bias_add(dc_bottleneck(tmp_rs_1,wc4_1,wc4_2,output_shape=tmpShape),bc4))\n",
    "    \n",
    "    tmp_add_1=tf.add(tmp_dc_1,tmp_cv_1_pooling)\n",
    "    tmp_add_1=tf.nn.dropout(tmp_add_1,keep_prob)\n",
    "    \n",
    "    tmp_rs_2=tf.image.resize_bilinear(tmp_add_1,tmp_cv_1.get_shape().as_list()[1:3])\n",
    "    tmpShape=img.get_shape().as_list()\n",
    "    tmpShape[0]=batch_size\n",
    "    tmp_dc_2=tf.nn.relu(tf.nn.bias_add(dc_bottleneck(tmp_rs_2,wc5_1,wc5_2,output_shape=tmpShape),bc5))\n",
    "    \n",
    "    tmp_add_2=tf.add(tmp_dc_2,img)\n",
    "    tmp_add_21=tf.nn.dropout(tmp_add_2,keep_prob)\n",
    "    \n",
    "    return (tmp_add_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc1=tf.Variable(tf.random_normal([7,7,3,16]),name=\"wc1\")\n",
    "bc1=tf.Variable(tf.random_normal([16]),name=\"bc1\")\n",
    "\n",
    "wh1_1_1=tf.Variable(tf.random_normal([5,5,16,64]),name=\"wh1_1_1\")\n",
    "wh1_1_2=tf.Variable(tf.random_normal([5,5,64,16]),name=\"wh1_1_2\")\n",
    "\n",
    "bh1_1=tf.Variable(tf.random_normal([16]),name=\"bh1_1\")\n",
    "\n",
    "wh1_2_1=tf.Variable(tf.random_normal([3,3,16,64]),name=\"wh1_2_1\")\n",
    "wh1_2_2=tf.Variable(tf.random_normal([3,3,64,16]),name=\"wh1_2_2\")\n",
    "\n",
    "bh1_2=tf.Variable(tf.random_normal([16]),name=\"bh1_2\")\n",
    "\n",
    "wh1_3_1=tf.Variable(tf.random_normal([3,3,16,16]),name=\"wh1_3_1\")\n",
    "wh1_3_2=tf.Variable(tf.random_normal([3,3,16,16]),name=\"wh1_3_2\")\n",
    "\n",
    "bh1_3=tf.Variable(tf.random_normal([16]),name=\"bh1_3\")\n",
    "\n",
    "wh1_4_1=tf.Variable(tf.random_normal([3,3,64,16]),name=\"wh1_4_1\")\n",
    "wh1_4_2=tf.Variable(tf.random_normal([3,3,16,64]),name=\"wh1_4_2\")\n",
    "\n",
    "bh1_4=tf.Variable(tf.random_normal([16]),name=\"bh1_4\")\n",
    "\n",
    "wh1_5_1=tf.Variable(tf.random_normal([3,3,64,16]),name=\"wh1_5_1\")\n",
    "wh1_5_2=tf.Variable(tf.random_normal([3,3,16,64]),name=\"wh1_5_2\")\n",
    "\n",
    "bh1_5=tf.Variable(tf.random_normal([16]),name=\"bh1_5\")\n",
    "\n",
    "wd1=tf.Variable(tf.random_normal([7,7,16,16]),name=\"wd1\")\n",
    "bd1=tf.Variable(tf.random_normal([16]),name=\"bd1\")\n",
    "\n",
    "wc2=tf.Variable(tf.random_normal([1,1,16,16]),name=\"wc2\")\n",
    "bc2=tf.Variable(tf.random_normal([16]),name=\"bc2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct model\n",
    "conv1=conv2d(x,wc1,bc1)\n",
    "conv1_pooling=max_pool(conv1,k=2)\n",
    "conv1_pooling=tf.nn.dropout(conv1_pooling,keep_prob)\n",
    "\n",
    "hg_1=hourglass(conv1_pooling,wh1_1_1,wh1_1_2,bh1_1,wh1_2_1,wh1_2_2,bh1_2,wh1_3_1,wh1_3_2,bh1_3,wh1_4_1,wh1_4_2,bh1_4,wh1_5_1,wh1_5_2,bh1_5)\n",
    "\n",
    "resize1=tf.image.resize_bilinear(hg_1,conv1.get_shape().as_list()[1:3])\n",
    "tmpShape=x.get_shape().as_list()\n",
    "tmpShape[0]=batch_size\n",
    "tmpShape[3]=wd1.get_shape().as_list()[2]\n",
    "dconv1=conv2d_transpose(resize1,wd1,bd1,output_shape=tmpShape)\n",
    "dconv1=tf.nn.dropout(dconv1,keep_prob)\n",
    "\n",
    "conv2=conv2d(dconv1,wc2,bc2)\n",
    "conv2=tf.nn.dropout(conv2,keep_prob)\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=conv2,labels=y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"wc1\":wc1,\"bc1\":bc1,\"wh1_1_1\":wh1_1_1,\"wh1_1_2\":wh1_1_2,\"bh1_1\":bh1_1,\"wh1_2_1\":wh1_2_1,\"wh1_2_2\":wh1_2_2,\n",
    "                        \"bh1_2\":bh1_2,\"wh1_3_1\":wh1_3_1,\"wh1_3_2\":wh1_3_2,\"bh1_3\":bh1_3,\"wh1_4_1\":wh1_4_1,\"wh1_4_2\":wh1_4_2,\n",
    "                        \"bh1_4\":bh1_4,\"wh1_5_1\":wh1_5_1,\"wh1_5_2\":wh1_5_2,\"bh1_5\":bh1_5,\n",
    "                        \"wd1\":wd1,\"bd1\":bd1,\"wc2\":wc2,\"bc2\":bc2\n",
    "                       })\n",
    "init=tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0       !!!!!!!!!!!!\n",
      "Iter 0, Minibatch Loss=819129116262400.000000\n",
      "Iter 2, Minibatch Loss=1284763499364352.000000\n",
      "Iter 4, Minibatch Loss=819087508766720.000000\n",
      "Iter 6, Minibatch Loss=549176496619520.000000\n",
      "Iter 8, Minibatch Loss=1728315241005056.000000\n",
      "Iter 10, Minibatch Loss=988739790700544.000000\n",
      "Iter 12, Minibatch Loss=291007354634240.000000\n",
      "Iter 14, Minibatch Loss=322687335399424.000000\n",
      "Iter 16, Minibatch Loss=872987200847872.000000\n",
      "Iter 18, Minibatch Loss=1102678025306112.000000\n",
      "Iter 20, Minibatch Loss=672247744299008.000000\n",
      "Iter 22, Minibatch Loss=1013015885381632.000000\n",
      "Iter 24, Minibatch Loss=887994688995328.000000\n",
      "Iter 26, Minibatch Loss=379253296201728.000000\n",
      "Iter 28, Minibatch Loss=506563039068160.000000\n",
      "Iter 30, Minibatch Loss=525926697795584.000000\n",
      "Iter 32, Minibatch Loss=371327303155712.000000\n",
      "Iter 34, Minibatch Loss=414719995281408.000000\n",
      "Iter 36, Minibatch Loss=1276310601072640.000000\n",
      "Iter 38, Minibatch Loss=1453826297036800.000000\n",
      "Iter 40, Minibatch Loss=1573756279455744.000000\n",
      "Iter 42, Minibatch Loss=1178449502797824.000000\n",
      "Iter 44, Minibatch Loss=1642831164735488.000000\n",
      "Iter 46, Minibatch Loss=1019128294932480.000000\n",
      "Iter 48, Minibatch Loss=813492642775040.000000\n",
      "Iter 50, Minibatch Loss=977165524926464.000000\n",
      "Iter 52, Minibatch Loss=814046827773952.000000\n",
      "Iter 54, Minibatch Loss=356752163864576.000000\n",
      "Iter 56, Minibatch Loss=437804672745472.000000\n",
      "Iter 58, Minibatch Loss=406871848517632.000000\n",
      "Iter 60, Minibatch Loss=545468933210112.000000\n",
      "Iter 62, Minibatch Loss=444122066321408.000000\n",
      "Iter 64, Minibatch Loss=442301973266432.000000\n",
      "Iter 66, Minibatch Loss=453481135603712.000000\n",
      "Iter 68, Minibatch Loss=526585606176768.000000\n",
      "Iter 70, Minibatch Loss=655066667155456.000000\n",
      "Iter 72, Minibatch Loss=424926414635008.000000\n",
      "Iter 74, Minibatch Loss=344541571842048.000000\n",
      "Iter 76, Minibatch Loss=779226588381184.000000\n",
      "Iter 78, Minibatch Loss=421523626131456.000000\n",
      "Iter 80, Minibatch Loss=174157635518464.000000\n",
      "Iter 82, Minibatch Loss=728891987591168.000000\n",
      "Iter 84, Minibatch Loss=561500905275392.000000\n",
      "Iter 86, Minibatch Loss=785080628805632.000000\n",
      "Iter 88, Minibatch Loss=832416335790080.000000\n",
      "Iter 90, Minibatch Loss=181156100177920.000000\n",
      "Iter 92, Minibatch Loss=329944219516928.000000\n",
      "Iter 94, Minibatch Loss=588218453983232.000000\n",
      "Iter 96, Minibatch Loss=999108781277184.000000\n",
      "Iter 98, Minibatch Loss=690379552718848.000000\n",
      "Iter 100, Minibatch Loss=239144919891968.000000\n",
      "Iter 102, Minibatch Loss=191701066973184.000000\n",
      "Iter 104, Minibatch Loss=372591365718016.000000\n",
      "Iter 106, Minibatch Loss=167657655500800.000000\n",
      "Iter 108, Minibatch Loss=425198507524096.000000\n",
      "Iter 110, Minibatch Loss=197774419165184.000000\n",
      "Iter 112, Minibatch Loss=249671196868608.000000\n",
      "Iter 114, Minibatch Loss=188287239061504.000000\n",
      "Iter 116, Minibatch Loss=582936650842112.000000\n",
      "Iter 118, Minibatch Loss=164414015668224.000000\n",
      "Iter 120, Minibatch Loss=269691750711296.000000\n",
      "Iter 122, Minibatch Loss=196898514272256.000000\n",
      "Iter 124, Minibatch Loss=322191568666624.000000\n",
      "Iter 126, Minibatch Loss=625968800595968.000000\n",
      "Iter 128, Minibatch Loss=596411573862400.000000\n",
      "Iter 130, Minibatch Loss=604084130283520.000000\n",
      "Iter 132, Minibatch Loss=362449773527040.000000\n",
      "Iter 134, Minibatch Loss=354038851829760.000000\n",
      "Iter 136, Minibatch Loss=379907473408000.000000\n",
      "Iter 138, Minibatch Loss=247983643820032.000000\n",
      "Iter 140, Minibatch Loss=127769321144320.000000\n",
      "Iter 142, Minibatch Loss=491706579419136.000000\n",
      "Iter 144, Minibatch Loss=907704293916672.000000\n",
      "Iter 146, Minibatch Loss=765562753908736.000000\n",
      "Iter 148, Minibatch Loss=856775309918208.000000\n",
      "Iter 150, Minibatch Loss=712658152062976.000000\n",
      "Iter 152, Minibatch Loss=586686828380160.000000\n",
      "Iter 154, Minibatch Loss=421387193810944.000000\n",
      "Iter 156, Minibatch Loss=164676981751808.000000\n",
      "Iter 158, Minibatch Loss=161746488655872.000000\n",
      "Iter 160, Minibatch Loss=227784647507968.000000\n",
      "Iter 162, Minibatch Loss=147387188248576.000000\n",
      "Iter 164, Minibatch Loss=115009719894016.000000\n",
      "Iter 166, Minibatch Loss=326761011216384.000000\n",
      "Iter 168, Minibatch Loss=243856431906816.000000\n",
      "Iter 170, Minibatch Loss=184379758346240.000000\n",
      "Iter 172, Minibatch Loss=170647892262912.000000\n",
      "Iter 174, Minibatch Loss=169067797282816.000000\n",
      "Iter 176, Minibatch Loss=208900330618880.000000\n",
      "Iter 178, Minibatch Loss=363802117799936.000000\n",
      "Iter 180, Minibatch Loss=841925393383424.000000\n",
      "Iter 182, Minibatch Loss=677945555288064.000000\n",
      "Iter 184, Minibatch Loss=1015417443188736.000000\n",
      "Iter 186, Minibatch Loss=1410489271713792.000000\n",
      "Iter 188, Minibatch Loss=596857646481408.000000\n",
      "Iter 190, Minibatch Loss=2068397193953280.000000\n",
      "Iter 192, Minibatch Loss=2040681769992192.000000\n",
      "Iter 194, Minibatch Loss=91898953859072.000000\n",
      "Iter 196, Minibatch Loss=83404406128640.000000\n",
      "Iter 198, Minibatch Loss=54391008657408.000000\n",
      "Iter 200, Minibatch Loss=57370071793664.000000\n",
      "Iter 202, Minibatch Loss=77875038388224.000000\n",
      "Iter 204, Minibatch Loss=85182874910720.000000\n",
      "Iter 206, Minibatch Loss=64226915778560.000000\n",
      "Iter 208, Minibatch Loss=38145307443200.000000\n",
      "Iter 210, Minibatch Loss=104387066200064.000000\n",
      "Iter 212, Minibatch Loss=77390185234432.000000\n",
      "Iter 214, Minibatch Loss=80029778182144.000000\n",
      "Iter 216, Minibatch Loss=62470643253248.000000\n",
      "Iter 218, Minibatch Loss=87542816833536.000000\n",
      "Iter 220, Minibatch Loss=74358290644992.000000\n",
      "Iter 222, Minibatch Loss=45399293296640.000000\n",
      "Iter 224, Minibatch Loss=449363235045376.000000\n",
      "Iter 226, Minibatch Loss=500763189051392.000000\n",
      "Iter 228, Minibatch Loss=324270399946752.000000\n",
      "Iter 230, Minibatch Loss=129321414950912.000000\n",
      "Iter 232, Minibatch Loss=91498599153664.000000\n",
      "Iter 234, Minibatch Loss=129619806126080.000000\n",
      "Iter 236, Minibatch Loss=108926125211648.000000\n",
      "Iter 238, Minibatch Loss=135007339282432.000000\n",
      "Iter 240, Minibatch Loss=144344891785216.000000\n",
      "Iter 242, Minibatch Loss=308860493496320.000000\n",
      "Iter 244, Minibatch Loss=1047052226134016.000000\n",
      "Iter 246, Minibatch Loss=1430876407726080.000000\n",
      "Iter 248, Minibatch Loss=920673417428992.000000\n",
      "Iter 250, Minibatch Loss=745377145815040.000000\n",
      "Iter 252, Minibatch Loss=589905033953280.000000\n",
      "Iter 254, Minibatch Loss=551373473054720.000000\n",
      "Iter 256, Minibatch Loss=720681889169408.000000\n",
      "Iter 258, Minibatch Loss=151216973676544.000000\n",
      "Iter 260, Minibatch Loss=744881211310080.000000\n",
      "Iter 262, Minibatch Loss=546145860321280.000000\n",
      "Iter 264, Minibatch Loss=1043245744259072.000000\n",
      "Iter 266, Minibatch Loss=426876296232960.000000\n",
      "Iter 268, Minibatch Loss=664278499590144.000000\n",
      "Iter 270, Minibatch Loss=196457525149696.000000\n",
      "Iter 272, Minibatch Loss=93933761724416.000000\n",
      "Iter 274, Minibatch Loss=122538688512000.000000\n",
      "Iter 276, Minibatch Loss=203996316827648.000000\n",
      "Iter 278, Minibatch Loss=201349224464384.000000\n",
      "Iter 280, Minibatch Loss=191627985420288.000000\n",
      "Iter 282, Minibatch Loss=279624114241536.000000\n",
      "Iter 284, Minibatch Loss=426567561904128.000000\n",
      "Iter 286, Minibatch Loss=310315279450112.000000\n",
      "Iter 288, Minibatch Loss=476812169707520.000000\n",
      "Iter 290, Minibatch Loss=421837964050432.000000\n",
      "Iter 292, Minibatch Loss=345199876243456.000000\n",
      "Iter 294, Minibatch Loss=305500956655616.000000\n",
      "Iter 296, Minibatch Loss=609498674757632.000000\n",
      "Iter 298, Minibatch Loss=349339654291456.000000\n",
      "Iter 300, Minibatch Loss=683540756824064.000000\n",
      "Iter 302, Minibatch Loss=818622041686016.000000\n",
      "Iter 304, Minibatch Loss=462450101059584.000000\n",
      "Iter 306, Minibatch Loss=671557596741632.000000\n",
      "Iter 308, Minibatch Loss=419841441792000.000000\n",
      "Iter 310, Minibatch Loss=617912649908224.000000\n",
      "Iter 312, Minibatch Loss=952315917893632.000000\n",
      "Iter 314, Minibatch Loss=393919401558016.000000\n",
      "Iter 316, Minibatch Loss=622476723748864.000000\n",
      "Iter 318, Minibatch Loss=584853816868864.000000\n",
      "Iter 320, Minibatch Loss=691130769342464.000000\n",
      "Iter 322, Minibatch Loss=909103178186752.000000\n",
      "Iter 324, Minibatch Loss=527478086959104.000000\n",
      "Iter 326, Minibatch Loss=509785573163008.000000\n",
      "Iter 328, Minibatch Loss=565243331739648.000000\n",
      "Iter 330, Minibatch Loss=620559155068928.000000\n",
      "Iter 332, Minibatch Loss=291269850955776.000000\n",
      "Iter 334, Minibatch Loss=634899111346176.000000\n",
      "Iter 336, Minibatch Loss=614190725201920.000000\n",
      "Iter 338, Minibatch Loss=577648673685504.000000\n",
      "Iter 340, Minibatch Loss=926250164027392.000000\n",
      "Iter 342, Minibatch Loss=1168712677720064.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 344, Minibatch Loss=823542329376768.000000\n",
      "Iter 346, Minibatch Loss=800133616762880.000000\n",
      "Iter 348, Minibatch Loss=1232086530785280.000000\n",
      "Iter 350, Minibatch Loss=1239011091808256.000000\n",
      "Iter 352, Minibatch Loss=1081340359344128.000000\n",
      "Iter 354, Minibatch Loss=584771474292736.000000\n",
      "Iter 356, Minibatch Loss=402963361169408.000000\n",
      "Iter 358, Minibatch Loss=316046376435712.000000\n",
      "Iter 360, Minibatch Loss=416821408694272.000000\n",
      "Iter 362, Minibatch Loss=414963667566592.000000\n",
      "Iter 364, Minibatch Loss=310490802683904.000000\n",
      "Iter 366, Minibatch Loss=256541634592768.000000\n",
      "Iter 368, Minibatch Loss=389223257473024.000000\n",
      "Iter 370, Minibatch Loss=795804759490560.000000\n",
      "Iter 372, Minibatch Loss=108604564701184.000000\n",
      "Iter 374, Minibatch Loss=75677801906176.000000\n",
      "Iter 376, Minibatch Loss=78092554993664.000000\n",
      "Iter 378, Minibatch Loss=650861155975168.000000\n",
      "Iter 380, Minibatch Loss=596161660452864.000000\n",
      "Iter 382, Minibatch Loss=1194149889966080.000000\n",
      "Iter 384, Minibatch Loss=383213021167616.000000\n",
      "Iter 386, Minibatch Loss=448618158882816.000000\n",
      "Iter 388, Minibatch Loss=297819608973312.000000\n",
      "Iter 390, Minibatch Loss=394760409841664.000000\n",
      "Iter 392, Minibatch Loss=382603504910336.000000\n",
      "Iter 394, Minibatch Loss=721060987142144.000000\n",
      "Iter 396, Minibatch Loss=320136896577536.000000\n",
      "Iter 398, Minibatch Loss=168575419547648.000000\n",
      "Iter 400, Minibatch Loss=706109803331584.000000\n",
      "Iter 402, Minibatch Loss=798776272879616.000000\n",
      "Iter 404, Minibatch Loss=977775343173632.000000\n",
      "Iter 406, Minibatch Loss=693424550313984.000000\n",
      "Iter 408, Minibatch Loss=501733784551424.000000\n",
      "Iter 410, Minibatch Loss=483672406884352.000000\n",
      "Iter 412, Minibatch Loss=1240396487196672.000000\n",
      "Iter 414, Minibatch Loss=1170136862031872.000000\n",
      "Iter 416, Minibatch Loss=580146331385856.000000\n",
      "Iter 418, Minibatch Loss=1099768587616256.000000\n",
      "Iter 420, Minibatch Loss=1137243318124544.000000\n",
      "Iter 422, Minibatch Loss=840476177465344.000000\n",
      "Iter 424, Minibatch Loss=648537108905984.000000\n",
      "Iter 426, Minibatch Loss=724006864945152.000000\n",
      "Iter 428, Minibatch Loss=1016187450294272.000000\n",
      "Iter 430, Minibatch Loss=1106895079211008.000000\n",
      "Iter 432, Minibatch Loss=975692686688256.000000\n",
      "Iter 434, Minibatch Loss=818427224653824.000000\n",
      "Iter 436, Minibatch Loss=468761958154240.000000\n",
      "Iter 438, Minibatch Loss=587146591207424.000000\n",
      "Iter 440, Minibatch Loss=688204151783424.000000\n",
      "Iter 442, Minibatch Loss=1027051033198592.000000\n",
      "Iter 444, Minibatch Loss=772634149126144.000000\n",
      "Iter 446, Minibatch Loss=755195105509376.000000\n",
      "Iter 448, Minibatch Loss=695841543159808.000000\n",
      "Iter 450, Minibatch Loss=605211089436672.000000\n",
      "Iter 452, Minibatch Loss=362251198398464.000000\n",
      "Iter 454, Minibatch Loss=548989933977600.000000\n",
      "Iter 456, Minibatch Loss=576834978709504.000000\n",
      "Iter 458, Minibatch Loss=1031459481583616.000000\n",
      "Iter 460, Minibatch Loss=163515612528640.000000\n",
      "Iter 462, Minibatch Loss=225362067849216.000000\n",
      "Iter 464, Minibatch Loss=375104458457088.000000\n",
      "Iter 466, Minibatch Loss=741382088032256.000000\n",
      "Iter 468, Minibatch Loss=338237298049024.000000\n",
      "Iter 470, Minibatch Loss=324414616895488.000000\n",
      "Iter 472, Minibatch Loss=470010988331008.000000\n",
      "Iter 474, Minibatch Loss=1091970403401728.000000\n",
      "Iter 476, Minibatch Loss=341785645678592.000000\n",
      "Iter 478, Minibatch Loss=249005795704832.000000\n",
      "Iter 480, Minibatch Loss=632830077960192.000000\n",
      "Iter 482, Minibatch Loss=966881024409600.000000\n",
      "Iter 484, Minibatch Loss=811346501304320.000000\n",
      "Iter 486, Minibatch Loss=924890873987072.000000\n",
      "Iter 488, Minibatch Loss=349465852510208.000000\n",
      "Iter 490, Minibatch Loss=999124484751360.000000\n",
      "Iter 492, Minibatch Loss=360339166199808.000000\n",
      "Iter 494, Minibatch Loss=330518168076288.000000\n",
      "Iter 496, Minibatch Loss=830253886865408.000000\n",
      "Iter 498, Minibatch Loss=757327355445248.000000\n",
      "epoch:1       !!!!!!!!!!!!\n",
      "Iter 0, Minibatch Loss=819079992573952.000000\n",
      "Iter 2, Minibatch Loss=1284689276960768.000000\n",
      "Iter 4, Minibatch Loss=819039257493504.000000\n",
      "Iter 6, Minibatch Loss=549143177068544.000000\n",
      "Iter 8, Minibatch Loss=1728212967096320.000000\n",
      "Iter 10, Minibatch Loss=988680265138176.000000\n",
      "Iter 12, Minibatch Loss=290989671448576.000000\n",
      "Iter 14, Minibatch Loss=322667303403520.000000\n",
      "Iter 16, Minibatch Loss=872933647974400.000000\n",
      "Iter 18, Minibatch Loss=1102611117768704.000000\n",
      "Iter 20, Minibatch Loss=672206673674240.000000\n",
      "Iter 22, Minibatch Loss=1012954480771072.000000\n",
      "Iter 24, Minibatch Loss=887940330815488.000000\n",
      "Iter 26, Minibatch Loss=379229774544896.000000\n",
      "Iter 28, Minibatch Loss=506531934109696.000000\n",
      "Iter 30, Minibatch Loss=525894720421888.000000\n",
      "Iter 32, Minibatch Loss=371305593438208.000000\n",
      "Iter 34, Minibatch Loss=414696070971392.000000\n",
      "Iter 36, Minibatch Loss=1276231815266304.000000\n",
      "Iter 38, Minibatch Loss=1453738652860416.000000\n",
      "Iter 40, Minibatch Loss=1573660582215680.000000\n",
      "Iter 42, Minibatch Loss=1178376488353792.000000\n",
      "Iter 44, Minibatch Loss=1642731038310400.000000\n",
      "Iter 46, Minibatch Loss=1019065346818048.000000\n",
      "Iter 48, Minibatch Loss=813442311127040.000000\n",
      "Iter 50, Minibatch Loss=977106066472960.000000\n",
      "Iter 52, Minibatch Loss=813997636976640.000000\n",
      "Iter 54, Minibatch Loss=356729783058432.000000\n",
      "Iter 56, Minibatch Loss=437777023893504.000000\n",
      "Iter 58, Minibatch Loss=406846179377152.000000\n",
      "Iter 60, Minibatch Loss=545434841907200.000000\n",
      "Iter 62, Minibatch Loss=444094518132736.000000\n",
      "Iter 64, Minibatch Loss=442274223751168.000000\n",
      "Iter 66, Minibatch Loss=453452278792192.000000\n",
      "Iter 68, Minibatch Loss=526553125486592.000000\n",
      "Iter 70, Minibatch Loss=655024992550912.000000\n",
      "Iter 72, Minibatch Loss=424899000664064.000000\n",
      "Iter 74, Minibatch Loss=344519493025792.000000\n",
      "Iter 76, Minibatch Loss=779177129148416.000000\n",
      "Iter 78, Minibatch Loss=421496447041536.000000\n",
      "Iter 80, Minibatch Loss=174146227011584.000000\n",
      "Iter 82, Minibatch Loss=728845414039552.000000\n",
      "Iter 84, Minibatch Loss=561465505349632.000000\n",
      "Iter 86, Minibatch Loss=785032713076736.000000\n",
      "Iter 88, Minibatch Loss=832363051352064.000000\n",
      "Iter 90, Minibatch Loss=181144255463424.000000\n",
      "Iter 92, Minibatch Loss=329922375581696.000000\n",
      "Iter 94, Minibatch Loss=588179799277568.000000\n",
      "Iter 96, Minibatch Loss=999043618570240.000000\n",
      "Iter 98, Minibatch Loss=690334254235648.000000\n",
      "Iter 100, Minibatch Loss=239128897650688.000000\n",
      "Iter 102, Minibatch Loss=191688400175104.000000\n",
      "Iter 104, Minibatch Loss=372566736764928.000000\n",
      "Iter 106, Minibatch Loss=167646381211648.000000\n",
      "Iter 108, Minibatch Loss=425170120474624.000000\n",
      "Iter 110, Minibatch Loss=197761030946816.000000\n",
      "Iter 112, Minibatch Loss=249654453207040.000000\n",
      "Iter 114, Minibatch Loss=188274639372288.000000\n",
      "Iter 116, Minibatch Loss=582897459265536.000000\n",
      "Iter 118, Minibatch Loss=164403009814528.000000\n",
      "Iter 120, Minibatch Loss=269673731981312.000000\n",
      "Iter 122, Minibatch Loss=196885260271616.000000\n",
      "Iter 124, Minibatch Loss=322170093830144.000000\n",
      "Iter 126, Minibatch Loss=625926790447104.000000\n",
      "Iter 128, Minibatch Loss=596371778306048.000000\n",
      "Iter 130, Minibatch Loss=604044267618304.000000\n",
      "Iter 132, Minibatch Loss=362425681444864.000000\n",
      "Iter 134, Minibatch Loss=354015195955200.000000\n",
      "Iter 136, Minibatch Loss=379882274029568.000000\n",
      "Iter 138, Minibatch Loss=247967437029376.000000\n",
      "Iter 140, Minibatch Loss=127760781541376.000000\n",
      "Iter 142, Minibatch Loss=491674568491008.000000\n",
      "Iter 144, Minibatch Loss=907644902572032.000000\n",
      "Iter 146, Minibatch Loss=765512892022784.000000\n",
      "Iter 148, Minibatch Loss=856720817520640.000000\n",
      "Iter 150, Minibatch Loss=712611846946816.000000\n",
      "Iter 152, Minibatch Loss=586647569694720.000000\n",
      "Iter 154, Minibatch Loss=421358471217152.000000\n",
      "Iter 156, Minibatch Loss=164665959120896.000000\n",
      "Iter 158, Minibatch Loss=161735650574336.000000\n",
      "Iter 160, Minibatch Loss=227768843370496.000000\n",
      "Iter 162, Minibatch Loss=147377272913920.000000\n",
      "Iter 164, Minibatch Loss=115002019151872.000000\n",
      "Iter 166, Minibatch Loss=326738965954560.000000\n",
      "Iter 168, Minibatch Loss=243839923126272.000000\n",
      "Iter 170, Minibatch Loss=184366772781056.000000\n",
      "Iter 172, Minibatch Loss=170636232097792.000000\n",
      "Iter 174, Minibatch Loss=169056573325312.000000\n",
      "Iter 176, Minibatch Loss=208886673965056.000000\n",
      "Iter 178, Minibatch Loss=363777321074688.000000\n",
      "Iter 180, Minibatch Loss=841870833876992.000000\n",
      "Iter 182, Minibatch Loss=677900927893504.000000\n",
      "Iter 184, Minibatch Loss=1015349998780416.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 186, Minibatch Loss=1410396258828288.000000\n",
      "Iter 188, Minibatch Loss=596817918033920.000000\n",
      "Iter 190, Minibatch Loss=2068264184184832.000000\n",
      "Iter 192, Minibatch Loss=2040548626006016.000000\n",
      "Iter 194, Minibatch Loss=91892544962560.000000\n",
      "Iter 196, Minibatch Loss=83398584434688.000000\n",
      "Iter 198, Minibatch Loss=54387221200896.000000\n",
      "Iter 200, Minibatch Loss=57366041067520.000000\n",
      "Iter 202, Minibatch Loss=77869510295552.000000\n",
      "Iter 204, Minibatch Loss=85176918999040.000000\n",
      "Iter 206, Minibatch Loss=64222486593536.000000\n",
      "Iter 208, Minibatch Loss=38142669225984.000000\n",
      "Iter 210, Minibatch Loss=104379768111104.000000\n",
      "Iter 212, Minibatch Loss=77384741027840.000000\n",
      "Iter 214, Minibatch Loss=80024283643904.000000\n",
      "Iter 216, Minibatch Loss=62466251816960.000000\n",
      "Iter 218, Minibatch Loss=87536651206656.000000\n",
      "Iter 220, Minibatch Loss=74353098096640.000000\n",
      "Iter 222, Minibatch Loss=45396109819904.000000\n",
      "Iter 224, Minibatch Loss=449333304492032.000000\n",
      "Iter 226, Minibatch Loss=500729164857344.000000\n",
      "Iter 228, Minibatch Loss=324248388239360.000000\n",
      "Iter 230, Minibatch Loss=129312573358080.000000\n",
      "Iter 232, Minibatch Loss=91492282531840.000000\n",
      "Iter 234, Minibatch Loss=129610847092736.000000\n",
      "Iter 236, Minibatch Loss=108918516744192.000000\n",
      "Iter 238, Minibatch Loss=134998078259200.000000\n",
      "Iter 240, Minibatch Loss=144335010004992.000000\n",
      "Iter 242, Minibatch Loss=308839857520640.000000\n",
      "Iter 244, Minibatch Loss=1046982432915456.000000\n",
      "Iter 246, Minibatch Loss=1430782589534208.000000\n",
      "Iter 248, Minibatch Loss=920612952342528.000000\n",
      "Iter 250, Minibatch Loss=745327351037952.000000\n",
      "Iter 252, Minibatch Loss=589866379247616.000000\n",
      "Iter 254, Minibatch Loss=551336194080768.000000\n",
      "Iter 256, Minibatch Loss=720632564154368.000000\n",
      "Iter 258, Minibatch Loss=151206739574784.000000\n",
      "Iter 260, Minibatch Loss=744832691601408.000000\n",
      "Iter 262, Minibatch Loss=546110359732224.000000\n",
      "Iter 264, Minibatch Loss=1043176420802560.000000\n",
      "Iter 266, Minibatch Loss=426847439421440.000000\n",
      "Iter 268, Minibatch Loss=664233939304448.000000\n",
      "Iter 270, Minibatch Loss=196444405366784.000000\n",
      "Iter 272, Minibatch Loss=93927361216512.000000\n",
      "Iter 274, Minibatch Loss=122530492841984.000000\n",
      "Iter 276, Minibatch Loss=203982576287744.000000\n",
      "Iter 278, Minibatch Loss=201335584587776.000000\n",
      "Iter 280, Minibatch Loss=191615033409536.000000\n",
      "Iter 282, Minibatch Loss=279605189541888.000000\n",
      "Iter 284, Minibatch Loss=426539376181248.000000\n",
      "Iter 286, Minibatch Loss=310294408593408.000000\n",
      "Iter 288, Minibatch Loss=476780360105984.000000\n",
      "Iter 290, Minibatch Loss=421810013208576.000000\n",
      "Iter 292, Minibatch Loss=345176891457536.000000\n",
      "Iter 294, Minibatch Loss=305480689778688.000000\n",
      "Iter 296, Minibatch Loss=609458610765824.000000\n",
      "Iter 298, Minibatch Loss=349316803723264.000000\n",
      "Iter 300, Minibatch Loss=683495525449728.000000\n",
      "Iter 302, Minibatch Loss=818566274220032.000000\n",
      "Iter 304, Minibatch Loss=462418559893504.000000\n",
      "Iter 306, Minibatch Loss=671513103564800.000000\n",
      "Iter 308, Minibatch Loss=419813121851392.000000\n",
      "Iter 310, Minibatch Loss=617871512174592.000000\n",
      "Iter 312, Minibatch Loss=952252030255104.000000\n",
      "Iter 314, Minibatch Loss=393893430427648.000000\n",
      "Iter 316, Minibatch Loss=622435653124096.000000\n",
      "Iter 318, Minibatch Loss=584815631925248.000000\n",
      "Iter 320, Minibatch Loss=691085537968128.000000\n",
      "Iter 322, Minibatch Loss=909042310447104.000000\n",
      "Iter 324, Minibatch Loss=527441781063680.000000\n",
      "Iter 326, Minibatch Loss=509751414751232.000000\n",
      "Iter 328, Minibatch Loss=565204945469440.000000\n",
      "Iter 330, Minibatch Loss=620517614682112.000000\n",
      "Iter 332, Minibatch Loss=291249416306688.000000\n",
      "Iter 334, Minibatch Loss=634857436741632.000000\n",
      "Iter 336, Minibatch Loss=614149386141696.000000\n",
      "Iter 338, Minibatch Loss=577609482108928.000000\n",
      "Iter 340, Minibatch Loss=926189296287744.000000\n",
      "Iter 342, Minibatch Loss=1168636173615104.000000\n",
      "Iter 344, Minibatch Loss=823488172523520.000000\n",
      "Iter 346, Minibatch Loss=800079594127360.000000\n",
      "Iter 348, Minibatch Loss=1232006000148480.000000\n",
      "Iter 350, Minibatch Loss=1238932708655104.000000\n",
      "Iter 352, Minibatch Loss=1081271438540800.000000\n",
      "Iter 354, Minibatch Loss=584731812954112.000000\n",
      "Iter 356, Minibatch Loss=402936014307328.000000\n",
      "Iter 358, Minibatch Loss=316024901599232.000000\n",
      "Iter 360, Minibatch Loss=416793692733440.000000\n",
      "Iter 362, Minibatch Loss=414935850942464.000000\n",
      "Iter 364, Minibatch Loss=310469998936064.000000\n",
      "Iter 366, Minibatch Loss=256524454723584.000000\n",
      "Iter 368, Minibatch Loss=389197017907200.000000\n",
      "Iter 370, Minibatch Loss=795750736855040.000000\n",
      "Iter 372, Minibatch Loss=108597056897024.000000\n",
      "Iter 374, Minibatch Loss=75672609357824.000000\n",
      "Iter 376, Minibatch Loss=78087152730112.000000\n",
      "Iter 378, Minibatch Loss=650818004975616.000000\n",
      "Iter 380, Minibatch Loss=596121328025600.000000\n",
      "Iter 382, Minibatch Loss=1194070835724288.000000\n",
      "Iter 384, Minibatch Loss=383186882265088.000000\n",
      "Iter 386, Minibatch Loss=448587758567424.000000\n",
      "Iter 388, Minibatch Loss=297799375650816.000000\n",
      "Iter 390, Minibatch Loss=394733364969472.000000\n",
      "Iter 392, Minibatch Loss=382577667997696.000000\n",
      "Iter 394, Minibatch Loss=721011796344832.000000\n",
      "Iter 396, Minibatch Loss=320115086196736.000000\n",
      "Iter 398, Minibatch Loss=168563994263552.000000\n",
      "Iter 400, Minibatch Loss=706062625800192.000000\n",
      "Iter 402, Minibatch Loss=798723793747968.000000\n",
      "Iter 404, Minibatch Loss=977709979140096.000000\n",
      "Iter 406, Minibatch Loss=693378513633280.000000\n",
      "Iter 408, Minibatch Loss=501699928129536.000000\n",
      "Iter 410, Minibatch Loss=483639993303040.000000\n",
      "Iter 412, Minibatch Loss=1240314211729408.000000\n",
      "Iter 414, Minibatch Loss=1170059284185088.000000\n",
      "Iter 416, Minibatch Loss=580107810897920.000000\n",
      "Iter 418, Minibatch Loss=1099696311369728.000000\n",
      "Iter 420, Minibatch Loss=1137168424632320.000000\n",
      "Iter 422, Minibatch Loss=840420141563904.000000\n",
      "Iter 424, Minibatch Loss=648493823688704.000000\n",
      "Iter 426, Minibatch Loss=723959418978304.000000\n",
      "Iter 428, Minibatch Loss=1016120878301184.000000\n",
      "Iter 430, Minibatch Loss=1106819917283328.000000\n",
      "Iter 432, Minibatch Loss=975627188436992.000000\n",
      "Iter 434, Minibatch Loss=818371725623296.000000\n",
      "Iter 436, Minibatch Loss=468730249216000.000000\n",
      "Iter 438, Minibatch Loss=587107802284032.000000\n",
      "Iter 440, Minibatch Loss=688157980884992.000000\n",
      "Iter 442, Minibatch Loss=1026984058552320.000000\n",
      "Iter 444, Minibatch Loss=772583481933824.000000\n",
      "Iter 446, Minibatch Loss=755144706752512.000000\n",
      "Iter 448, Minibatch Loss=695794902499328.000000\n",
      "Iter 450, Minibatch Loss=605170287247360.000000\n",
      "Iter 452, Minibatch Loss=362226535890944.000000\n",
      "Iter 454, Minibatch Loss=548953460310016.000000\n",
      "Iter 456, Minibatch Loss=576796458221568.000000\n",
      "Iter 458, Minibatch Loss=1031391298977792.000000\n",
      "Iter 460, Minibatch Loss=163504472457216.000000\n",
      "Iter 462, Minibatch Loss=225347001909248.000000\n",
      "Iter 464, Minibatch Loss=375079292633088.000000\n",
      "Iter 466, Minibatch Loss=741332830126080.000000\n",
      "Iter 468, Minibatch Loss=338215286341632.000000\n",
      "Iter 470, Minibatch Loss=324392504524800.000000\n",
      "Iter 472, Minibatch Loss=469979279392768.000000\n",
      "Iter 474, Minibatch Loss=1091898529808384.000000\n",
      "Iter 476, Minibatch Loss=341762895773696.000000\n",
      "Iter 478, Minibatch Loss=248988968157184.000000\n",
      "Iter 480, Minibatch Loss=632788738899968.000000\n",
      "Iter 482, Minibatch Loss=966817874968576.000000\n",
      "Iter 484, Minibatch Loss=811293418192896.000000\n",
      "Iter 486, Minibatch Loss=924829469376512.000000\n",
      "Iter 488, Minibatch Loss=349442330853376.000000\n",
      "Iter 490, Minibatch Loss=999058181193728.000000\n",
      "Iter 492, Minibatch Loss=360315308998656.000000\n",
      "Iter 494, Minibatch Loss=330496055705600.000000\n",
      "Iter 496, Minibatch Loss=830198857596928.000000\n",
      "Iter 498, Minibatch Loss=757276755361792.000000\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    #saver.restore(sess,\"model/model.ckpt\")\n",
    "    for i in range(0,epoch):\n",
    "        print(\"epoch:\"+str(i)+\"       !!!!!!!!!!!!\")\n",
    "        step=0\n",
    "        now_at=0\n",
    "        while step*batch_size<training_iters:\n",
    "            X=input_data.x_next_batch(\n",
    "                    img_dir_path='mpii_human_pose_v1\\\\output_images360x480\\\\',\n",
    "                    index_path='train_data\\\\new_data.json',\n",
    "                    img_height=img_height,img_width=img_width,\n",
    "                    batch_size=batch_size,now_at=now_at)\n",
    "            Y=input_data.y_next_batch(\n",
    "                    img_dir_path='mpii_human_pose_v1\\\\output_images360x480\\\\',\n",
    "                    index_path='train_data\\\\new_data.json',\n",
    "                    img_height=img_height,img_width=img_width,\n",
    "                    batch_size=batch_size,now_at=now_at)\n",
    "            sess.run(optimizer,feed_dict = {x:X,y:Y,keep_prob:dropout})\n",
    "            loss=sess.run(cost,feed_dict = {x:X,y:Y,keep_prob:1.})\n",
    "            if step%display_step==0:\n",
    "                print(\"Iter \"+str(step*batch_size)+\", Minibatch Loss=\"+\"{:.6f}\".format(loss))\n",
    "            step+=1\n",
    "            now_at+=batch_size\n",
    "        save_path = saver.save(sess, \"model/model_hg.ckpt\")\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
